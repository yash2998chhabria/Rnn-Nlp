{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapedDatasetRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPA1/Musb+B61dvRScJMFiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash2998chhabria/Rnn-Nlp/blob/master/ScrapedDatasetRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfJwFvTrLUOB",
        "outputId": "66ad6f01-123b-4bbf-f440-11ed69f046b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwxeRu5DMYte"
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEbACTB4Lde9"
      },
      "source": [
        "Combined_df = pd.read_csv(\"/content/drive/MyDrive/depressionrnn/scrapedtotaldataset.csv\") "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DZfQPkOlMVbJ",
        "outputId": "39631556-0f48-4ac6-d393-abc1df638b90"
      },
      "source": [
        "Combined_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It started with feeling irritated over small i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>At 16 years old I sat in my first therapy sess...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Depression is debilitating.,Some people unders...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Depression can be a face of someone who is smi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For a long while, I've been having issues with...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>&amp;nbsp;\\r\\n\\r\\n*[Previous chapter](https://www....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1366</th>\n",
              "      <td>\\[[first](https://www.reddit.com/r/HFY/comment...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1367</th>\n",
              "      <td>\\[[first](https://www.reddit.com/r/HFY/comment...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1368</th>\n",
              "      <td>\\[[first](https://www.reddit.com/r/HFY/comment...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1369</th>\n",
              "      <td>\\[[first](https://www.reddit.com/r/HFY/comment...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1370 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  story  condition\n",
              "0     It started with feeling irritated over small i...          1\n",
              "1     At 16 years old I sat in my first therapy sess...          1\n",
              "2     Depression is debilitating.,Some people unders...          1\n",
              "3     Depression can be a face of someone who is smi...          1\n",
              "4     For a long while, I've been having issues with...          1\n",
              "...                                                 ...        ...\n",
              "1365  &nbsp;\\r\\n\\r\\n*[Previous chapter](https://www....          0\n",
              "1366  \\[[first](https://www.reddit.com/r/HFY/comment...          0\n",
              "1367  \\[[first](https://www.reddit.com/r/HFY/comment...          0\n",
              "1368  \\[[first](https://www.reddit.com/r/HFY/comment...          0\n",
              "1369  \\[[first](https://www.reddit.com/r/HFY/comment...          0\n",
              "\n",
              "[1370 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeUmRE3qMkXR"
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "import torchtext\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import re"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD7WyM_5Mof8"
      },
      "source": [
        "tweets = Combined_df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BEEyPbZtMpyr",
        "outputId": "b856ea0c-c4c1-498d-95c2-1cb5cd70b038"
      },
      "source": [
        "tweets = tweets.rename(index = str, columns = {'story': 'SentimentText', 'condition': 'Sentiment'})\n",
        "\n",
        "tweets.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It started with feeling irritated over small i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>At 16 years old I sat in my first therapy sess...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Depression is debilitating.,Some people unders...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Depression can be a face of someone who is smi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For a long while, I've been having issues with...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  It started with feeling irritated over small i...          1\n",
              "1  At 16 years old I sat in my first therapy sess...          1\n",
              "2  Depression is debilitating.,Some people unders...          1\n",
              "3  Depression can be a face of someone who is smi...          1\n",
              "4  For a long while, I've been having issues with...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3eI1i8pMrIO"
      },
      "source": [
        "tweets=tweets.dropna()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbf9hMgnMxYC",
        "outputId": "bbd8c268-5354-4817-e71a-861cf85c5086"
      },
      "source": [
        "tweets.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1368, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-pBTDrYMyjk",
        "outputId": "90fe70eb-e7b4-4adc-eeae-da6f273878ed"
      },
      "source": [
        "tweets['Sentiment'].unique()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrVj3bA2Mz6k",
        "outputId": "36c5bf60-eef5-420e-e5d3-ad475905d711"
      },
      "source": [
        "tweets.Sentiment.value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    699\n",
              "0    669\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "wQoze4SAM1Yb",
        "outputId": "99dde901-6028-40b3-c5cc-15deb8b2a43a"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
        "\n",
        "ax.set(xlabel='Labels')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 0, 'Labels')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo84wJXLM3w5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzmmRKxNM6cE",
        "outputId": "e2d16f3e-29f5-46c1-9c67-8a248e3c9860"
      },
      "source": [
        "train.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                          SentimentText  Sentiment\n",
              " 0     I have struggled with , , and , for the past 2...          1\n",
              " 1       When I was 18 and still in high school, I we...          0\n",
              " 2     There are seminal moments, defining moments in...          1\n",
              " 3     Ok, so today I went to a zoom meeting about cl...          0\n",
              " 4     Everything has limits, except for humans.  Or ...          0\n",
              " ...                                                 ...        ...\n",
              " 1089  Just wanted to post an update for the fraud Pl...          0\n",
              " 1090  https://www.reddit.com/r/legaladvice/comments/...          0\n",
              " 1091  &nbsp;\\r\\n\\r\\n*[Previous chapter](https://www....          0\n",
              " 1092  As a kid we would go to upstate New York on va...          0\n",
              " 1093  California here, getting that out of the way. ...          0\n",
              " \n",
              " [1094 rows x 2 columns],\n",
              "                                          SentimentText  Sentiment\n",
              " 0    OCD... Oh yeah... OCD is about neatness and ti...          1\n",
              " 1    This afternoon I found out girls have been kic...          0\n",
              " 2    To say that my life has been a upward challeng...          1\n",
              " 3    These are the words which you worry you’ll com...          1\n",
              " 4    It’s so important to surround yourself with , ...          1\n",
              " ..                                                 ...        ...\n",
              " 269  My dad passed away last week while at work. He...          0\n",
              " 270  I f(18) have only told 4 people about the even...          0\n",
              " 271  My mum and dad were diagnosed with , and ,, re...          1\n",
              " 272  Nick died tragically early in January 2015. He...          1\n",
              " 273  So I'm at a party and I'm doing okay. I've man...          1\n",
              " \n",
              " [274 rows x 2 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j54Z8zkQM7wl",
        "outputId": "7110f363-5d27-4c95-801c-0e90051c1947"
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1094, 2), (274, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rovvY6dUM9t6"
      },
      "source": [
        "train.to_csv('/content/drive/MyDrive/depressionrnn/train_tweets.csv', index=False)\n",
        "test.to_csv('/content/drive/MyDrive/depressionrnn/test_tweets.csv', index=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qsTsJpqNXgu"
      },
      "source": [
        "def tweet_clean(text):\n",
        "    \n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
        "    \n",
        "    return text.strip()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwZtVSU6NZM5"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "def tokenizer(s): \n",
        "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CApt5MRcNazA"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
        "\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp39GHAVNcIO"
      },
      "source": [
        "datafields = [('SentimentText', TEXT),('Sentiment', LABEL)]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uubrkjDNdP3"
      },
      "source": [
        "trn, tst = torchtext.data.TabularDataset.splits(path = '/content/drive/MyDrive/depressionrnn/', \n",
        "                                                train = 'train_tweets.csv',\n",
        "                                                test = 'test_tweets.csv',    \n",
        "                                                format = 'csv',\n",
        "                                                skip_header = True,\n",
        "                                                fields = datafields)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBYfJ2DmNeiM",
        "outputId": "5e84cac3-ea72-4cc5-86d7-723c40cc9e88"
      },
      "source": [
        "print(f'Number of training examples: {len(trn)}')\n",
        "print(f'Number of testing examples: {len(tst)}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 1094\n",
            "Number of testing examples: 274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40SoCqzeNh33",
        "outputId": "f48e3a26-936c-4258-a1d6-72f07ab1c120"
      },
      "source": [
        "vars(trn.examples[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '1',\n",
              " 'SentimentText': ['i',\n",
              "  'have',\n",
              "  'struggled',\n",
              "  'with',\n",
              "  'and',\n",
              "  'for',\n",
              "  'the',\n",
              "  'past',\n",
              "  '2',\n",
              "  'years',\n",
              "  'during',\n",
              "  'this',\n",
              "  'time',\n",
              "  'i',\n",
              "  'also',\n",
              "  'had',\n",
              "  'a',\n",
              "  'baby',\n",
              "  'and',\n",
              "  'i',\n",
              "  'am',\n",
              "  'also',\n",
              "  'a',\n",
              "  'busy',\n",
              "  'mum',\n",
              "  'to',\n",
              "  'my',\n",
              "  '15',\n",
              "  'year',\n",
              "  'old',\n",
              "  'daughter',\n",
              "  'i',\n",
              "  've',\n",
              "  'also',\n",
              "  'had',\n",
              "  'lots',\n",
              "  'of',\n",
              "  'support',\n",
              "  'from',\n",
              "  'my',\n",
              "  'husband',\n",
              "  'a',\n",
              "  'team',\n",
              "  'of',\n",
              "  'healthcare',\n",
              "  'professionals',\n",
              "  'family',\n",
              "  'and',\n",
              "  'friends',\n",
              "  'being',\n",
              "  'involved',\n",
              "  'with',\n",
              "  'the',\n",
              "  'campaign',\n",
              "  'and',\n",
              "  'attending',\n",
              "  'a',\n",
              "  'recovery',\n",
              "  'college',\n",
              "  'has',\n",
              "  'really',\n",
              "  'motivated',\n",
              "  'me',\n",
              "  'in',\n",
              "  'fact',\n",
              "  'i',\n",
              "  'am',\n",
              "  'recovering',\n",
              "  'well',\n",
              "  'that',\n",
              "  'i',\n",
              "  'felt',\n",
              "  'it',\n",
              "  'was',\n",
              "  'time',\n",
              "  'to',\n",
              "  'give',\n",
              "  'myself',\n",
              "  'a',\n",
              "  'daily',\n",
              "  'challenge',\n",
              "  'every',\n",
              "  'day',\n",
              "  'i',\n",
              "  'try',\n",
              "  'to',\n",
              "  'talk',\n",
              "  'briefly',\n",
              "  'to',\n",
              "  'a',\n",
              "  'random',\n",
              "  'stranger',\n",
              "  'about',\n",
              "  'the',\n",
              "  'difficulties',\n",
              "  'i',\n",
              "  'have',\n",
              "  'faced',\n",
              "  'and',\n",
              "  'to',\n",
              "  'talk',\n",
              "  'about',\n",
              "  'mental',\n",
              "  'health',\n",
              "  'today',\n",
              "  'it',\n",
              "  'was',\n",
              "  'a',\n",
              "  'lady',\n",
              "  'in',\n",
              "  'the',\n",
              "  'supermarket',\n",
              "  'but',\n",
              "  'last',\n",
              "  'friday',\n",
              "  'in',\n",
              "  'was',\n",
              "  'a',\n",
              "  'children',\n",
              "  's',\n",
              "  'entertainer',\n",
              "  'i',\n",
              "  'imagine',\n",
              "  'that',\n",
              "  'if',\n",
              "  'i',\n",
              "  'had',\n",
              "  'a',\n",
              "  'badly',\n",
              "  'broken',\n",
              "  'arm',\n",
              "  'with',\n",
              "  'pins',\n",
              "  'in',\n",
              "  'it',\n",
              "  'the',\n",
              "  'random',\n",
              "  'stranger',\n",
              "  'might',\n",
              "  'ask',\n",
              "  'me',\n",
              "  'how',\n",
              "  'i',\n",
              "  'did',\n",
              "  'it',\n",
              "  'or',\n",
              "  'if',\n",
              "  'it',\n",
              "  's',\n",
              "  'getting',\n",
              "  'better',\n",
              "  'it',\n",
              "  's',\n",
              "  'not',\n",
              "  'very',\n",
              "  'easy',\n",
              "  'starting',\n",
              "  'the',\n",
              "  'conversation',\n",
              "  'and',\n",
              "  'sometimes',\n",
              "  'i',\n",
              "  'do',\n",
              "  'get',\n",
              "  'the',\n",
              "  'odd',\n",
              "  'really',\n",
              "  'or',\n",
              "  'oh',\n",
              "  'dear',\n",
              "  'but',\n",
              "  'generally',\n",
              "  'i',\n",
              "  'find',\n",
              "  'people',\n",
              "  'are',\n",
              "  'happy',\n",
              "  'to',\n",
              "  'have',\n",
              "  'a',\n",
              "  'short',\n",
              "  'conversation',\n",
              "  'so',\n",
              "  'what',\n",
              "  'do',\n",
              "  'we',\n",
              "  'all',\n",
              "  'get',\n",
              "  'out',\n",
              "  'of',\n",
              "  'it',\n",
              "  'well',\n",
              "  'it',\n",
              "  's',\n",
              "  'my',\n",
              "  'way',\n",
              "  'of',\n",
              "  'getting',\n",
              "  'people',\n",
              "  'to',\n",
              "  'talk',\n",
              "  'about',\n",
              "  'mental',\n",
              "  'health',\n",
              "  'which',\n",
              "  'i',\n",
              "  'hope',\n",
              "  'will',\n",
              "  'make',\n",
              "  'a',\n",
              "  'tiny',\n",
              "  'difference',\n",
              "  'to',\n",
              "  'combat',\n",
              "  'the',\n",
              "  'stigma',\n",
              "  'around',\n",
              "  'the',\n",
              "  'subject',\n",
              "  'and',\n",
              "  'for',\n",
              "  'them',\n",
              "  'to',\n",
              "  'be',\n",
              "  'a',\n",
              "  'little',\n",
              "  'more',\n",
              "  'enlightened',\n",
              "  'and',\n",
              "  'perhaps',\n",
              "  'more',\n",
              "  'comfortable',\n",
              "  'to',\n",
              "  'talk',\n",
              "  'to',\n",
              "  'loved',\n",
              "  'ones',\n",
              "  'colleagues',\n",
              "  'or',\n",
              "  'friends',\n",
              "  'about',\n",
              "  'mental',\n",
              "  'health',\n",
              "  'talking',\n",
              "  'to',\n",
              "  'people',\n",
              "  'makes',\n",
              "  'me',\n",
              "  'feel',\n",
              "  'empowered',\n",
              "  'and',\n",
              "  'is',\n",
              "  'one',\n",
              "  'of',\n",
              "  'ways',\n",
              "  'i',\n",
              "  'am',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'thank',\n",
              "  'those',\n",
              "  'of',\n",
              "  'who',\n",
              "  'helped',\n",
              "  'and',\n",
              "  'supported',\n",
              "  'me',\n",
              "  'i',\n",
              "  'am',\n",
              "  'gaining',\n",
              "  'confidence',\n",
              "  'and',\n",
              "  'no',\n",
              "  'longer',\n",
              "  'feel',\n",
              "  'ashamed',\n",
              "  'about',\n",
              "  'my',\n",
              "  'mental',\n",
              "  'health',\n",
              "  'so',\n",
              "  'last',\n",
              "  'friday',\n",
              "  's',\n",
              "  'conversation',\n",
              "  'went',\n",
              "  'something',\n",
              "  'like',\n",
              "  'this',\n",
              "  'magician',\n",
              "  'man',\n",
              "  'asks',\n",
              "  'have',\n",
              "  'you',\n",
              "  'ever',\n",
              "  'been',\n",
              "  'to',\n",
              "  'winchester',\n",
              "  'before',\n",
              "  'i',\n",
              "  'reply',\n",
              "  'actually',\n",
              "  'i',\n",
              "  'lived',\n",
              "  'here',\n",
              "  'for',\n",
              "  'three',\n",
              "  'months',\n",
              "  'last',\n",
              "  'year',\n",
              "  'only',\n",
              "  'three',\n",
              "  'months',\n",
              "  'he',\n",
              "  'replies',\n",
              "  'that',\n",
              "  'was',\n",
              "  'a',\n",
              "  'short',\n",
              "  'stay',\n",
              "  'well',\n",
              "  'too',\n",
              "  'be',\n",
              "  'honest',\n",
              "  'i',\n",
              "  'was',\n",
              "  'in',\n",
              "  'a',\n",
              "  'specialist',\n",
              "  'psychiatric',\n",
              "  'unit',\n",
              "  'just',\n",
              "  'up',\n",
              "  'the',\n",
              "  'road',\n",
              "  'so',\n",
              "  'that',\n",
              "  's',\n",
              "  'why',\n",
              "  'i',\n",
              "  'was',\n",
              "  'here',\n",
              "  'magician',\n",
              "  'man',\n",
              "  'says',\n",
              "  'wow',\n",
              "  'i',\n",
              "  'm',\n",
              "  'sorry',\n",
              "  'to',\n",
              "  'hear',\n",
              "  'that',\n",
              "  'are',\n",
              "  'you',\n",
              "  'better',\n",
              "  'now',\n",
              "  'and',\n",
              "  'so',\n",
              "  'we',\n",
              "  'chat',\n",
              "  'for',\n",
              "  'about',\n",
              "  '5',\n",
              "  'minutes',\n",
              "  'he',\n",
              "  'tells',\n",
              "  'me',\n",
              "  'about',\n",
              "  'a',\n",
              "  'family',\n",
              "  'member',\n",
              "  'who',\n",
              "  'had',\n",
              "  'been',\n",
              "  'unwell',\n",
              "  'and',\n",
              "  'i',\n",
              "  'tell',\n",
              "  'him',\n",
              "  'about',\n",
              "  'my',\n",
              "  'experiences',\n",
              "  'and',\n",
              "  'how',\n",
              "  'i',\n",
              "  'am',\n",
              "  'recovering',\n",
              "  'job',\n",
              "  'done',\n",
              "  'for',\n",
              "  'today',\n",
              "  'comment',\n",
              "  'below',\n",
              "  'or',\n",
              "  'and',\n",
              "  'find',\n",
              "  'out']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCafx3-8NyOV",
        "outputId": "3ee83c80-197a-4bd3-ff15-138cb2ed2787"
      },
      "source": [
        "vars(tst.examples[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '1',\n",
              " 'SentimentText': ['ocd',\n",
              "  'oh',\n",
              "  'yeah',\n",
              "  'ocd',\n",
              "  'is',\n",
              "  'about',\n",
              "  'neatness',\n",
              "  'and',\n",
              "  'tidiness',\n",
              "  'isn',\n",
              "  't',\n",
              "  'it',\n",
              "  'it',\n",
              "  's',\n",
              "  'about',\n",
              "  'keeping',\n",
              "  'things',\n",
              "  'in',\n",
              "  'order',\n",
              "  'it',\n",
              "  's',\n",
              "  'washing',\n",
              "  'your',\n",
              "  'hands',\n",
              "  'a',\n",
              "  'lot',\n",
              "  'this',\n",
              "  'was',\n",
              "  'my',\n",
              "  'and',\n",
              "  'i',\n",
              "  'm',\n",
              "  'sure',\n",
              "  'many',\n",
              "  'other',\n",
              "  'people',\n",
              "  's',\n",
              "  'view',\n",
              "  'of',\n",
              "  'ocd',\n",
              "  'in',\n",
              "  'fact',\n",
              "  'i',\n",
              "  'make',\n",
              "  'fun',\n",
              "  'of',\n",
              "  'myself',\n",
              "  'a',\n",
              "  'lot',\n",
              "  'because',\n",
              "  'i',\n",
              "  'am',\n",
              "  'incredibly',\n",
              "  'untidy',\n",
              "  'i',\n",
              "  'wish',\n",
              "  'i',\n",
              "  'had',\n",
              "  'the',\n",
              "  'tidiness',\n",
              "  'part',\n",
              "  'of',\n",
              "  'ocd',\n",
              "  'i',\n",
              "  'claim',\n",
              "  'but',\n",
              "  'i',\n",
              "  'know',\n",
              "  'that',\n",
              "  'really',\n",
              "  'i',\n",
              "  'wish',\n",
              "  'i',\n",
              "  'didn',\n",
              "  't',\n",
              "  'experience',\n",
              "  'this',\n",
              "  'condition',\n",
              "  'at',\n",
              "  'all',\n",
              "  'people',\n",
              "  'laugh',\n",
              "  'about',\n",
              "  'the',\n",
              "  'compulsions',\n",
              "  'that',\n",
              "  'can',\n",
              "  'come',\n",
              "  'with',\n",
              "  'ocd',\n",
              "  'i',\n",
              "  'know',\n",
              "  'i',\n",
              "  'have',\n",
              "  'done',\n",
              "  'it',\n",
              "  'myself',\n",
              "  'with',\n",
              "  'a',\n",
              "  'laugh',\n",
              "  'they',\n",
              "  'say',\n",
              "  'oh',\n",
              "  'i',\n",
              "  've',\n",
              "  'got',\n",
              "  'that',\n",
              "  'i',\n",
              "  'check',\n",
              "  'the',\n",
              "  'door',\n",
              "  'all',\n",
              "  'the',\n",
              "  'time',\n",
              "  'or',\n",
              "  'i',\n",
              "  'know',\n",
              "  'what',\n",
              "  'you',\n",
              "  'mean',\n",
              "  'i',\n",
              "  'm',\n",
              "  'a',\n",
              "  'neat',\n",
              "  'freak',\n",
              "  'this',\n",
              "  'makes',\n",
              "  'any',\n",
              "  'explanation',\n",
              "  'harder',\n",
              "  'in',\n",
              "  'some',\n",
              "  'ways',\n",
              "  'because',\n",
              "  'they',\n",
              "  'may',\n",
              "  'just',\n",
              "  'do',\n",
              "  'these',\n",
              "  'things',\n",
              "  'with',\n",
              "  'no',\n",
              "  'compulsion',\n",
              "  'involved',\n",
              "  'i',\n",
              "  'have',\n",
              "  'not',\n",
              "  'told',\n",
              "  'many',\n",
              "  'people',\n",
              "  'about',\n",
              "  'the',\n",
              "  'ocd',\n",
              "  'thing',\n",
              "  'lots',\n",
              "  'of',\n",
              "  'people',\n",
              "  'know',\n",
              "  'that',\n",
              "  'i',\n",
              "  'have',\n",
              "  'and',\n",
              "  'but',\n",
              "  'when',\n",
              "  'people',\n",
              "  'think',\n",
              "  'that',\n",
              "  'ocd',\n",
              "  'is',\n",
              "  'all',\n",
              "  'about',\n",
              "  'tidiness',\n",
              "  'because',\n",
              "  'that',\n",
              "  's',\n",
              "  'what',\n",
              "  'tv',\n",
              "  'shows',\n",
              "  'them',\n",
              "  'then',\n",
              "  'how',\n",
              "  'can',\n",
              "  'i',\n",
              "  'explain',\n",
              "  'my',\n",
              "  'ocd',\n",
              "  'i',\n",
              "  'worry',\n",
              "  'that',\n",
              "  'people',\n",
              "  'will',\n",
              "  'not',\n",
              "  'believe',\n",
              "  'me',\n",
              "  'because',\n",
              "  'if',\n",
              "  'i',\n",
              "  'was',\n",
              "  'washing',\n",
              "  'my',\n",
              "  'hands',\n",
              "  'or',\n",
              "  'keeping',\n",
              "  'my',\n",
              "  'stuff',\n",
              "  'majorly',\n",
              "  'tidy',\n",
              "  'then',\n",
              "  'you',\n",
              "  'would',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'see',\n",
              "  'it',\n",
              "  'my',\n",
              "  'ocd',\n",
              "  'is',\n",
              "  'hidden',\n",
              "  'in',\n",
              "  'my',\n",
              "  'head',\n",
              "  'and',\n",
              "  'leaves',\n",
              "  'me',\n",
              "  'with',\n",
              "  'this',\n",
              "  'crippling',\n",
              "  'anxiety',\n",
              "  'so',\n",
              "  'that',\n",
              "  'i',\n",
              "  'can',\n",
              "  't',\n",
              "  'function',\n",
              "  'properly',\n",
              "  'the',\n",
              "  'people',\n",
              "  'that',\n",
              "  'i',\n",
              "  'have',\n",
              "  'told',\n",
              "  'are',\n",
              "  'the',\n",
              "  'people',\n",
              "  'that',\n",
              "  'i',\n",
              "  'knew',\n",
              "  'would',\n",
              "  'make',\n",
              "  'the',\n",
              "  'big',\n",
              "  'effort',\n",
              "  'to',\n",
              "  'understand',\n",
              "  'anyway',\n",
              "  'the',\n",
              "  'people',\n",
              "  'that',\n",
              "  'have',\n",
              "  'been',\n",
              "  'there',\n",
              "  'for',\n",
              "  'me',\n",
              "  'when',\n",
              "  'i',\n",
              "  'have',\n",
              "  'been',\n",
              "  'off',\n",
              "  'work',\n",
              "  'with',\n",
              "  'anxiety',\n",
              "  'and',\n",
              "  'the',\n",
              "  'people',\n",
              "  'who',\n",
              "  'ask',\n",
              "  'how',\n",
              "  'i',\n",
              "  'am',\n",
              "  'and',\n",
              "  'care',\n",
              "  'about',\n",
              "  'the',\n",
              "  'answer',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'it',\n",
              "  'requires',\n",
              "  'a',\n",
              "  'huge',\n",
              "  'explanation',\n",
              "  'puts',\n",
              "  'me',\n",
              "  'off',\n",
              "  'i',\n",
              "  'have',\n",
              "  'to',\n",
              "  'explain',\n",
              "  'the',\n",
              "  'condition',\n",
              "  'first',\n",
              "  'that',\n",
              "  'it',\n",
              "  'is',\n",
              "  'about',\n",
              "  'obsessing',\n",
              "  'and',\n",
              "  'compulsions',\n",
              "  'not',\n",
              "  'that',\n",
              "  'it',\n",
              "  'is',\n",
              "  'about',\n",
              "  'tidiness',\n",
              "  'then',\n",
              "  'move',\n",
              "  'on',\n",
              "  'to',\n",
              "  'how',\n",
              "  'it',\n",
              "  'is',\n",
              "  'for',\n",
              "  'me',\n",
              "  'these',\n",
              "  'people',\n",
              "  'have',\n",
              "  'been',\n",
              "  'ace',\n",
              "  'but',\n",
              "  'i',\n",
              "  'wish',\n",
              "  'i',\n",
              "  'could',\n",
              "  'be',\n",
              "  'more',\n",
              "  'open',\n",
              "  'about',\n",
              "  'it',\n",
              "  'it',\n",
              "  's',\n",
              "  'the',\n",
              "  'fear',\n",
              "  'of',\n",
              "  'people',\n",
              "  'thinking',\n",
              "  'i',\n",
              "  'should',\n",
              "  'just',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'ignore',\n",
              "  'it',\n",
              "  'all',\n",
              "  'and',\n",
              "  'the',\n",
              "  'lack',\n",
              "  'of',\n",
              "  'understanding',\n",
              "  'of',\n",
              "  'what',\n",
              "  'ocd',\n",
              "  'can',\n",
              "  'be',\n",
              "  'that',\n",
              "  'puts',\n",
              "  'me',\n",
              "  'off',\n",
              "  'ideally',\n",
              "  'i',\n",
              "  'would',\n",
              "  'love',\n",
              "  'to',\n",
              "  'be',\n",
              "  'able',\n",
              "  'to',\n",
              "  'explain',\n",
              "  'to',\n",
              "  'more',\n",
              "  'people',\n",
              "  'and',\n",
              "  'all',\n",
              "  'i',\n",
              "  'ask',\n",
              "  'is',\n",
              "  'that',\n",
              "  'they',\n",
              "  'do',\n",
              "  'what',\n",
              "  'the',\n",
              "  'people',\n",
              "  'i',\n",
              "  'have',\n",
              "  'already',\n",
              "  'told',\n",
              "  'did',\n",
              "  'put',\n",
              "  'their',\n",
              "  'previous',\n",
              "  'experience',\n",
              "  'to',\n",
              "  'one',\n",
              "  'side',\n",
              "  'and',\n",
              "  'listen',\n",
              "  'to',\n",
              "  'how',\n",
              "  'it',\n",
              "  'is',\n",
              "  'for',\n",
              "  'me',\n",
              "  'it',\n",
              "  'has',\n",
              "  'taken',\n",
              "  'me',\n",
              "  'until',\n",
              "  'this',\n",
              "  'year',\n",
              "  'to',\n",
              "  'realise',\n",
              "  'that',\n",
              "  'ocd',\n",
              "  'is',\n",
              "  'what',\n",
              "  'i',\n",
              "  'have',\n",
              "  'been',\n",
              "  'living',\n",
              "  'with',\n",
              "  'for',\n",
              "  'the',\n",
              "  'last',\n",
              "  '24',\n",
              "  'years',\n",
              "  'i',\n",
              "  'always',\n",
              "  'thought',\n",
              "  'i',\n",
              "  'was',\n",
              "  'simply',\n",
              "  'rubbish',\n",
              "  'at',\n",
              "  'enjoying',\n",
              "  'things',\n",
              "  'at',\n",
              "  'living',\n",
              "  'in',\n",
              "  'the',\n",
              "  'moment',\n",
              "  'at',\n",
              "  'being',\n",
              "  'sociable',\n",
              "  'with',\n",
              "  'others',\n",
              "  'but',\n",
              "  'what',\n",
              "  'i',\n",
              "  'didn',\n",
              "  't',\n",
              "  'know',\n",
              "  'was',\n",
              "  'that',\n",
              "  'the',\n",
              "  'constant',\n",
              "  'buzz',\n",
              "  'in',\n",
              "  'my',\n",
              "  'head',\n",
              "  'is',\n",
              "  'not',\n",
              "  'normal',\n",
              "  'or',\n",
              "  'at',\n",
              "  'least',\n",
              "  'it',\n",
              "  's',\n",
              "  'not',\n",
              "  'what',\n",
              "  'everyone',\n",
              "  'else',\n",
              "  'experiences',\n",
              "  'this',\n",
              "  'is',\n",
              "  'because',\n",
              "  'i',\n",
              "  'understood',\n",
              "  'ocd',\n",
              "  'in',\n",
              "  'the',\n",
              "  'same',\n",
              "  'way',\n",
              "  'most',\n",
              "  'others',\n",
              "  'did',\n",
              "  'cleaning',\n",
              "  'tidying',\n",
              "  'ordering',\n",
              "  'until',\n",
              "  'i',\n",
              "  'saw',\n",
              "  'one',\n",
              "  'tv',\n",
              "  'programme',\n",
              "  'that',\n",
              "  'showed',\n",
              "  'me',\n",
              "  'different',\n",
              "  'my',\n",
              "  'head',\n",
              "  'is',\n",
              "  'like',\n",
              "  'a',\n",
              "  'horrible',\n",
              "  'person',\n",
              "  'picking',\n",
              "  'up',\n",
              "  'on',\n",
              "  'all',\n",
              "  'the',\n",
              "  'negative',\n",
              "  'feelings',\n",
              "  'i',\n",
              "  'have',\n",
              "  'about',\n",
              "  'myself',\n",
              "  'and',\n",
              "  'obsessively',\n",
              "  'bombarding',\n",
              "  'me',\n",
              "  'with',\n",
              "  'them',\n",
              "  'i',\n",
              "  'am',\n",
              "  'told',\n",
              "  'how',\n",
              "  'rubbish',\n",
              "  'i',\n",
              "  'am',\n",
              "  'how',\n",
              "  'sad',\n",
              "  'how',\n",
              "  'nobody',\n",
              "  'wants',\n",
              "  'anything',\n",
              "  'to',\n",
              "  'do',\n",
              "  'with',\n",
              "  'me',\n",
              "  'and',\n",
              "  'that',\n",
              "  'i',\n",
              "  'will',\n",
              "  'never',\n",
              "  'be',\n",
              "  'normal',\n",
              "  'whatever',\n",
              "  'that',\n",
              "  'means',\n",
              "  'this',\n",
              "  'isn',\n",
              "  't',\n",
              "  'a',\n",
              "  'voice',\n",
              "  'in',\n",
              "  'my',\n",
              "  'head',\n",
              "  'it',\n",
              "  's',\n",
              "  'me',\n",
              "  'telling',\n",
              "  'me',\n",
              "  'which',\n",
              "  'is',\n",
              "  'why',\n",
              "  'it',\n",
              "  'is',\n",
              "  'nearly',\n",
              "  'impossible',\n",
              "  'to',\n",
              "  'fight',\n",
              "  'i',\n",
              "  'compulsively',\n",
              "  'pick',\n",
              "  'away',\n",
              "  'at',\n",
              "  'myself',\n",
              "  'i',\n",
              "  'question',\n",
              "  'everything',\n",
              "  'and',\n",
              "  'if',\n",
              "  'i',\n",
              "  'can',\n",
              "  'see',\n",
              "  'the',\n",
              "  'negative',\n",
              "  'in',\n",
              "  'something',\n",
              "  'then',\n",
              "  'i',\n",
              "  'do',\n",
              "  'this',\n",
              "  'is',\n",
              "  'not',\n",
              "  'low',\n",
              "  'self',\n",
              "  'esteem',\n",
              "  'this',\n",
              "  'is',\n",
              "  'obsessive',\n",
              "  'worrying',\n",
              "  'and',\n",
              "  'compulsive',\n",
              "  'thinking',\n",
              "  'causing',\n",
              "  'pain',\n",
              "  'i',\n",
              "  'can',\n",
              "  't',\n",
              "  'just',\n",
              "  'ignore',\n",
              "  'them',\n",
              "  'all',\n",
              "  'the',\n",
              "  'time',\n",
              "  'i',\n",
              "  'am',\n",
              "  'not',\n",
              "  'giving',\n",
              "  'in',\n",
              "  'to',\n",
              "  'them',\n",
              "  'it',\n",
              "  'hurts',\n",
              "  'and',\n",
              "  'it',\n",
              "  's',\n",
              "  'hard',\n",
              "  'i',\n",
              "  'am',\n",
              "  'writing',\n",
              "  'this',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hope',\n",
              "  'that',\n",
              "  'other',\n",
              "  'people',\n",
              "  'might',\n",
              "  'identify',\n",
              "  'with',\n",
              "  'what',\n",
              "  'i',\n",
              "  'am',\n",
              "  'talking',\n",
              "  'about',\n",
              "  'though',\n",
              "  'i',\n",
              "  'wouldn',\n",
              "  't',\n",
              "  'wish',\n",
              "  'this',\n",
              "  'on',\n",
              "  'anyone',\n",
              "  'and',\n",
              "  'because',\n",
              "  'i',\n",
              "  'think',\n",
              "  'the',\n",
              "  'more',\n",
              "  'people',\n",
              "  'speak',\n",
              "  'out',\n",
              "  'and',\n",
              "  'support',\n",
              "  'each',\n",
              "  'other',\n",
              "  'the',\n",
              "  'easier',\n",
              "  'it',\n",
              "  'may',\n",
              "  'become',\n",
              "  'also',\n",
              "  'i',\n",
              "  'want',\n",
              "  'people',\n",
              "  'to',\n",
              "  'understand',\n",
              "  'that',\n",
              "  'it',\n",
              "  'would',\n",
              "  'be',\n",
              "  'great',\n",
              "  'if',\n",
              "  'people',\n",
              "  'could',\n",
              "  'remember',\n",
              "  'that',\n",
              "  'it',\n",
              "  'stands',\n",
              "  'for',\n",
              "  'obsessive',\n",
              "  'compulsive',\n",
              "  'disorder',\n",
              "  'and',\n",
              "  'not',\n",
              "  'obsessive',\n",
              "  'cleaning',\n",
              "  'disorder']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6POKqpSMN1T8",
        "outputId": "fba8a500-2c58-48c0-95e1-df737a29f679"
      },
      "source": [
        "TEXT.build_vocab(trn, max_size=25000,\n",
        "                 vectors=\"glove.6B.100d\",\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(trn)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                          \n",
            " 99%|█████████▉| 397857/400000 [00:20<00:00, 21308.40it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ_Ju966N5Cj",
        "outputId": "02f9bebb-fdc6-4896-c6e3-3aaf7ee46860"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(50))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 40082), ('i', 30574), ('to', 28783), ('and', 25635), ('a', 20110), ('of', 18317), ('that', 13403), ('it', 13368), ('was', 13157), ('my', 12158), ('in', 11162), ('you', 7148), ('me', 7146), ('t', 6976), ('with', 6841), ('for', 6784), ('had', 6434), ('they', 6027), ('as', 5840), ('he', 5790), ('is', 5736), ('on', 5697), ('but', 5459), ('s', 5358), ('have', 5251), ('this', 4878), ('be', 4785), ('we', 4565), ('not', 4408), ('at', 4393), ('from', 3741), ('his', 3708), ('so', 3671), ('she', 3600), ('about', 3484), ('out', 3342), ('were', 3331), ('what', 3268), ('all', 3198), ('her', 3136), ('one', 3081), ('their', 3064), ('when', 3026), ('would', 3008), ('or', 2990), ('up', 2972), ('are', 2957), ('them', 2930), ('can', 2781), ('there', 2756)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6L0qN49N6ov",
        "outputId": "99372f9b-4c1a-472b-a38c-ec5da5163a20"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', 'i', 'to', 'and', 'a', 'of', 'that', 'it']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1Rv3ZTpN7vb",
        "outputId": "c1a32cdd-9e7a-47e5-a57a-81f1488466a6"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f7b0ff39ea0>, {'1': 0, '0': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn3mdhjeN886"
      },
      "source": [
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "                                (trn, tst),\n",
        "                                batch_size = 64,\n",
        "                                sort_key=lambda x: len(x.SentimentText),\n",
        "                                sort_within_batch=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaCgbT8PN-l7"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "       \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4iuH_dlOACx"
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "\n",
        "dropout = 0.5"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ii8AgofOBhX"
      },
      "source": [
        "model = RNN(input_dim, \n",
        "            embedding_dim, \n",
        "            hidden_dim, \n",
        "            output_dim, \n",
        "            n_layers, \n",
        "            bidirectional, \n",
        "            dropout)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY0UvepBOC7a",
        "outputId": "8cd963d1-cd6c-484e-bfd8-2d2fa596bdc6"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_ph_qxUOEDt",
        "outputId": "c95e6642-4b4d-4c45-9cfb-e1c75a0113e9"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2264, -0.1698, -0.5443,  ..., -0.8675,  1.5221,  1.0766],\n",
              "        [ 0.0685,  1.7735,  1.5197,  ...,  2.0164, -0.5793,  0.1839],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.0458, -0.2206, -0.4131,  ...,  0.0164, -0.4578,  0.1915],\n",
              "        [-0.6544, -0.7541, -2.9852,  ...,  1.0020,  0.0176,  0.5351],\n",
              "        [-0.4073, -0.4381, -0.7940,  ...,  0.4838, -0.5865,  0.7650]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0fhwnAcOFbs",
        "outputId": "311b074a-88b3-4f12-f391-d7639582dbe4"
      },
      "source": [
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.0458, -0.2206, -0.4131,  ...,  0.0164, -0.4578,  0.1915],\n",
            "        [-0.6544, -0.7541, -2.9852,  ...,  1.0020,  0.0176,  0.5351],\n",
            "        [-0.4073, -0.4381, -0.7940,  ...,  0.4838, -0.5865,  0.7650]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo3vS_0fOG1Z"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.0005)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR5NdIA6OIH2"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum() / len(correct)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os3b1FMPOJup",
        "outputId": "ab95ea67-5703-40d2-d4ca-47b8fe134933"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "     \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r 99%|█████████▉| 397857/400000 [00:40<00:00, 21308.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.691 | Train Acc: 53.44% |\n",
            "| Epoch: 02 | Train Loss: 0.689 | Train Acc: 52.55% |\n",
            "| Epoch: 03 | Train Loss: 0.680 | Train Acc: 56.08% |\n",
            "| Epoch: 04 | Train Loss: 0.673 | Train Acc: 60.62% |\n",
            "| Epoch: 05 | Train Loss: 0.669 | Train Acc: 61.98% |\n",
            "| Epoch: 06 | Train Loss: 0.668 | Train Acc: 60.13% |\n",
            "| Epoch: 07 | Train Loss: 0.668 | Train Acc: 59.92% |\n",
            "| Epoch: 08 | Train Loss: 0.656 | Train Acc: 62.01% |\n",
            "| Epoch: 09 | Train Loss: 0.639 | Train Acc: 63.86% |\n",
            "| Epoch: 10 | Train Loss: 0.634 | Train Acc: 65.25% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyMreksSOMOP",
        "outputId": "abfdfb24-87b4-4673-ef1b-60abcd0b0dd8"
      },
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "\n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum()/len(correct)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "test_loss = epoch_loss / len(test_iterator)\n",
        "test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.617 | Test Acc: 62.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZZfS1yFUixf",
        "outputId": "e6b98e2f-0f75-475d-c0a3-a0aa80da5a58"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "     \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.625 | Train Acc: 66.09% |\n",
            "| Epoch: 02 | Train Loss: 0.593 | Train Acc: 72.83% |\n",
            "| Epoch: 03 | Train Loss: 0.565 | Train Acc: 73.26% |\n",
            "| Epoch: 04 | Train Loss: 0.526 | Train Acc: 77.34% |\n",
            "| Epoch: 05 | Train Loss: 0.431 | Train Acc: 83.36% |\n",
            "| Epoch: 06 | Train Loss: 0.353 | Train Acc: 89.15% |\n",
            "| Epoch: 07 | Train Loss: 0.335 | Train Acc: 89.35% |\n",
            "| Epoch: 08 | Train Loss: 0.292 | Train Acc: 91.49% |\n",
            "| Epoch: 09 | Train Loss: 0.238 | Train Acc: 94.53% |\n",
            "| Epoch: 10 | Train Loss: 0.247 | Train Acc: 92.88% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6JYo3xWUCS0",
        "outputId": "53cc1b50-dc2d-423e-d340-5145389010d0"
      },
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "\n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum()/len(correct)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "test_loss = epoch_loss / len(test_iterator)\n",
        "test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.153 | Test Acc: 96.25%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyJjV8IVUnt_",
        "outputId": "9aa59544-ab3a-4dfb-cfbb-2cd053789d63"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('embedding.weight',\n",
              "              tensor([[-3.1935e-02,  2.2140e-02,  2.3107e-02,  ...,  2.3594e-02,\n",
              "                        3.0229e-02,  2.2598e-04],\n",
              "                      [ 2.2199e-03,  5.8569e-03, -1.8207e-03,  ..., -2.1064e-03,\n",
              "                        6.8077e-03,  1.6938e-03],\n",
              "                      [-6.6674e-02, -2.4372e-01,  7.0432e-01,  ..., -9.0916e-02,\n",
              "                        8.5999e-01,  2.8466e-01],\n",
              "                      ...,\n",
              "                      [ 4.5810e-02, -2.2063e-01, -4.1309e-01,  ...,  1.6359e-02,\n",
              "                       -4.5778e-01,  1.9150e-01],\n",
              "                      [-6.5444e-01, -7.5406e-01, -2.9852e+00,  ...,  1.0020e+00,\n",
              "                        1.7642e-02,  5.3512e-01],\n",
              "                      [-4.0729e-01, -4.3813e-01, -7.9400e-01,  ...,  4.8382e-01,\n",
              "                       -5.8655e-01,  7.6498e-01]])),\n",
              "             ('rnn.weight_ih_l0',\n",
              "              tensor([[-0.0630,  0.1034,  0.2011,  ..., -0.0432,  0.0407,  0.0789],\n",
              "                      [-0.0115, -0.0671, -0.1935,  ..., -0.0573, -0.1343, -0.0780],\n",
              "                      [-0.2060,  0.0108, -0.0292,  ...,  0.1603, -0.0016,  0.2046],\n",
              "                      ...,\n",
              "                      [ 0.0552,  0.0931,  0.1678,  ..., -0.1386,  0.2119, -0.1693],\n",
              "                      [ 0.1269, -0.0572,  0.0117,  ..., -0.1802,  0.0506,  0.1585],\n",
              "                      [ 0.0538, -0.2113, -0.1622,  ..., -0.0837, -0.0406,  0.1690]])),\n",
              "             ('rnn.weight_hh_l0',\n",
              "              tensor([[-0.0364,  0.1989, -0.1579,  ...,  0.2111,  0.1855, -0.2127],\n",
              "                      [ 0.0300, -0.0850, -0.2236,  ...,  0.1387,  0.2327,  0.0378],\n",
              "                      [-0.0276,  0.0186,  0.0999,  ..., -0.0750, -0.0760,  0.1660],\n",
              "                      ...,\n",
              "                      [-0.2218,  0.1704,  0.0518,  ..., -0.2471,  0.1402, -0.0147],\n",
              "                      [ 0.1385, -0.2060,  0.0242,  ..., -0.1662, -0.0004, -0.1052],\n",
              "                      [-0.1632,  0.1655, -0.1637,  ...,  0.1257, -0.1474,  0.0374]])),\n",
              "             ('rnn.bias_ih_l0',\n",
              "              tensor([-0.1813,  0.1266,  0.1060, -0.1438,  0.1091,  0.0477, -0.1104,  0.0967,\n",
              "                       0.1392,  0.1268, -0.1244,  0.0469, -0.0217, -0.0090, -0.1420, -0.2259,\n",
              "                      -0.0077, -0.0829,  0.0267, -0.2249, -0.0409, -0.1076, -0.1331,  0.0628,\n",
              "                       0.0647, -0.0923,  0.1553, -0.2227, -0.1241, -0.0893,  0.0378, -0.0466,\n",
              "                       0.2391, -0.1913, -0.0512, -0.1295, -0.2164,  0.1609, -0.0332, -0.1167,\n",
              "                       0.1946, -0.1638, -0.0600,  0.0882, -0.0553,  0.0577,  0.0755,  0.0092,\n",
              "                       0.1897,  0.1982,  0.1950, -0.1140,  0.1381,  0.2003,  0.1017,  0.0445,\n",
              "                       0.1287,  0.0785,  0.1259, -0.1651])),\n",
              "             ('rnn.bias_hh_l0',\n",
              "              tensor([-0.1056, -0.0434, -0.0145, -0.0034, -0.2202,  0.0297, -0.0703,  0.1054,\n",
              "                      -0.0309, -0.1209,  0.2333,  0.0245,  0.1313,  0.1516,  0.0519,  0.0264,\n",
              "                      -0.0578, -0.0534, -0.0789, -0.2335,  0.1467,  0.0420,  0.2064, -0.0589,\n",
              "                       0.0115, -0.0702, -0.0527, -0.0434,  0.1933, -0.0463,  0.1745, -0.1999,\n",
              "                       0.1354, -0.0163,  0.1325, -0.0896,  0.0148,  0.1621,  0.0887,  0.0678,\n",
              "                       0.1494,  0.1841, -0.1245, -0.0965, -0.0985,  0.0511,  0.1667, -0.0104,\n",
              "                      -0.0718, -0.1311,  0.1802,  0.1119,  0.1919,  0.0953,  0.1711,  0.2047,\n",
              "                       0.1465,  0.1416,  0.0445,  0.0901])),\n",
              "             ('rnn.weight_ih_l0_reverse',\n",
              "              tensor([[ 0.1488,  0.1783,  0.1722,  ..., -0.0807, -0.1068,  0.0424],\n",
              "                      [ 0.2054, -0.2014,  0.1380,  ...,  0.1859, -0.1938, -0.0680],\n",
              "                      [-0.0381,  0.2054, -0.1389,  ..., -0.1351,  0.1110,  0.1926],\n",
              "                      ...,\n",
              "                      [-0.1693,  0.1184,  0.1893,  ..., -0.0149,  0.1344, -0.0388],\n",
              "                      [ 0.0747, -0.1996,  0.0707,  ..., -0.1453, -0.0670,  0.1133],\n",
              "                      [ 0.0301, -0.1235,  0.2368,  ..., -0.0363, -0.0171,  0.0233]])),\n",
              "             ('rnn.weight_hh_l0_reverse',\n",
              "              tensor([[ 0.0298,  0.2215,  0.0678,  ...,  0.0356,  0.0675,  0.2369],\n",
              "                      [ 0.0645, -0.2205,  0.2048,  ...,  0.2409, -0.1574,  0.1019],\n",
              "                      [ 0.0759,  0.1579, -0.0992,  ..., -0.0928,  0.1099, -0.0938],\n",
              "                      ...,\n",
              "                      [ 0.1181, -0.0925, -0.2306,  ..., -0.2095, -0.0232,  0.2223],\n",
              "                      [-0.1752,  0.1705,  0.1040,  ..., -0.0640, -0.1746, -0.0255],\n",
              "                      [ 0.0130, -0.1457,  0.0609,  ...,  0.2294, -0.0747, -0.0141]])),\n",
              "             ('rnn.bias_ih_l0_reverse',\n",
              "              tensor([-0.0359,  0.2069,  0.1761, -0.1671, -0.2037,  0.0109, -0.0164, -0.0617,\n",
              "                       0.1101, -0.0610,  0.1251,  0.0029,  0.0582,  0.0247,  0.1091, -0.0443,\n",
              "                      -0.0711,  0.2692,  0.0389,  0.0024, -0.0156,  0.0879, -0.1558,  0.0864,\n",
              "                       0.1798, -0.1064,  0.2049, -0.0994,  0.2249,  0.2155, -0.1615, -0.1218,\n",
              "                      -0.0432,  0.2098,  0.1225, -0.0049, -0.1553, -0.0808, -0.0377,  0.1656,\n",
              "                       0.0272,  0.0070, -0.0799, -0.1124,  0.1972,  0.1975, -0.0449, -0.0907,\n",
              "                      -0.0537,  0.0758, -0.1775,  0.0044, -0.1468,  0.0507,  0.2364, -0.0170,\n",
              "                       0.1312,  0.1993, -0.0207,  0.1400])),\n",
              "             ('rnn.bias_hh_l0_reverse',\n",
              "              tensor([-0.0146,  0.0116,  0.2485, -0.0880,  0.0396,  0.1362, -0.1516, -0.0708,\n",
              "                      -0.0140,  0.1564,  0.1855,  0.0213, -0.1090,  0.0542, -0.1058, -0.0855,\n",
              "                       0.1497,  0.1812, -0.1053, -0.1668,  0.2719, -0.0279,  0.1415,  0.0478,\n",
              "                      -0.2126,  0.0850,  0.1011, -0.1085, -0.0577,  0.0552, -0.0977,  0.2014,\n",
              "                       0.2164, -0.1725,  0.0671, -0.1546,  0.0149,  0.2196, -0.0025, -0.0983,\n",
              "                      -0.2419,  0.2027, -0.2003, -0.1161,  0.1993, -0.2158, -0.1950,  0.1485,\n",
              "                      -0.0057,  0.1716, -0.1929,  0.0890, -0.1964, -0.1952, -0.0024,  0.1364,\n",
              "                       0.1395,  0.0983, -0.1858, -0.1325])),\n",
              "             ('rnn.weight_ih_l1',\n",
              "              tensor([[ 0.0689,  0.0641,  0.0483,  ...,  0.0602,  0.1568,  0.0520],\n",
              "                      [ 0.1254,  0.2069, -0.1101,  ..., -0.0299,  0.2097, -0.1261],\n",
              "                      [-0.1994,  0.1306,  0.0321,  ...,  0.2084, -0.0177, -0.0549],\n",
              "                      ...,\n",
              "                      [-0.1479, -0.0777,  0.0873,  ...,  0.1078,  0.1812, -0.1391],\n",
              "                      [ 0.1503,  0.2012, -0.1387,  ...,  0.0806, -0.1525, -0.2148],\n",
              "                      [ 0.0306, -0.0574, -0.0261,  ..., -0.2095,  0.0008,  0.0865]])),\n",
              "             ('rnn.weight_hh_l1',\n",
              "              tensor([[-0.0116,  0.1241,  0.0620,  ..., -0.1685,  0.0845, -0.0847],\n",
              "                      [ 0.1409,  0.0186, -0.2137,  ..., -0.1756, -0.0566,  0.0839],\n",
              "                      [-0.1467, -0.1856,  0.2156,  ..., -0.0703, -0.0289,  0.2020],\n",
              "                      ...,\n",
              "                      [ 0.2169,  0.0149, -0.0262,  ...,  0.2202, -0.0977, -0.0708],\n",
              "                      [ 0.0046, -0.1557, -0.2193,  ...,  0.2172, -0.1722,  0.0409],\n",
              "                      [ 0.1530,  0.1564, -0.0247,  ...,  0.1870, -0.2121,  0.1089]])),\n",
              "             ('rnn.bias_ih_l1',\n",
              "              tensor([-0.1828, -0.2215, -0.1997, -0.1913,  0.0656, -0.0426, -0.0759,  0.0216,\n",
              "                      -0.1519, -0.1323,  0.1526, -0.1107, -0.1391, -0.0729,  0.1515,  0.1503,\n",
              "                      -0.2000, -0.0592,  0.0574,  0.1879,  0.1678, -0.2063, -0.1822,  0.1250,\n",
              "                      -0.2166,  0.1646,  0.0483,  0.0822,  0.1729,  0.0368, -0.0376,  0.1926,\n",
              "                      -0.0838, -0.0734,  0.0841, -0.0033,  0.0965, -0.0854, -0.1572, -0.0118,\n",
              "                       0.1470, -0.1416,  0.1753, -0.1091, -0.1315,  0.1743, -0.1774,  0.1417,\n",
              "                      -0.0624,  0.0352, -0.0805,  0.2195, -0.0213,  0.1475,  0.0710,  0.0241,\n",
              "                      -0.0652,  0.1632,  0.0423,  0.1085])),\n",
              "             ('rnn.bias_hh_l1',\n",
              "              tensor([ 0.1820, -0.0431,  0.1882,  0.0767,  0.0790, -0.0075,  0.0124, -0.1786,\n",
              "                      -0.0999,  0.0880, -0.2017,  0.0754, -0.1589,  0.0328,  0.0141,  0.0914,\n",
              "                      -0.1362,  0.1655,  0.2133, -0.1231, -0.2075,  0.2089, -0.0672, -0.1298,\n",
              "                      -0.0452,  0.0820, -0.1536,  0.1365, -0.1656, -0.1398,  0.1314,  0.0574,\n",
              "                      -0.2000,  0.1666, -0.0729, -0.0260, -0.0337,  0.2118,  0.1802, -0.0296,\n",
              "                       0.2386, -0.1561, -0.0061, -0.2041,  0.1066, -0.0844,  0.0727,  0.0670,\n",
              "                      -0.0733,  0.0582, -0.1226, -0.1584,  0.0185,  0.0827, -0.0477, -0.1045,\n",
              "                      -0.0803,  0.1069,  0.2071,  0.1030])),\n",
              "             ('rnn.weight_ih_l1_reverse',\n",
              "              tensor([[ 0.0223, -0.1643,  0.1000,  ..., -0.0608, -0.0094, -0.0215],\n",
              "                      [ 0.1973,  0.2553, -0.2896,  ...,  0.2227, -0.0998, -0.0347],\n",
              "                      [ 0.1526,  0.1849, -0.0776,  ..., -0.0213, -0.1855,  0.1588],\n",
              "                      ...,\n",
              "                      [-0.0817,  0.1713, -0.0495,  ...,  0.1523,  0.0126,  0.0480],\n",
              "                      [ 0.1812,  0.0076,  0.1727,  ...,  0.0594, -0.1568,  0.0267],\n",
              "                      [-0.1175, -0.2202,  0.0754,  ...,  0.1975,  0.0897, -0.2836]])),\n",
              "             ('rnn.weight_hh_l1_reverse',\n",
              "              tensor([[-0.0458,  0.0067,  0.2221,  ..., -0.2136, -0.1387,  0.0414],\n",
              "                      [ 0.0011, -0.0231, -0.2136,  ...,  0.2446, -0.1070,  0.1349],\n",
              "                      [-0.1636,  0.2847, -0.0287,  ..., -0.1124, -0.0046,  0.1997],\n",
              "                      ...,\n",
              "                      [-0.1478,  0.0380, -0.1468,  ..., -0.0613,  0.0037,  0.0054],\n",
              "                      [-0.0856,  0.0897,  0.1878,  ..., -0.1569,  0.0745, -0.0334],\n",
              "                      [-0.1852,  0.1029, -0.2366,  ...,  0.2787, -0.0996, -0.0694]])),\n",
              "             ('rnn.bias_ih_l1_reverse',\n",
              "              tensor([ 0.0038,  0.1554, -0.1161, -0.0172,  0.1661,  0.0877, -0.0277, -0.0127,\n",
              "                       0.2825,  0.0218, -0.0179,  0.2367, -0.0278,  0.0069, -0.0595,  0.0350,\n",
              "                      -0.0665,  0.0055,  0.1571, -0.0277, -0.0470, -0.0251, -0.1197, -0.2047,\n",
              "                      -0.0746,  0.0341, -0.1965,  0.1861,  0.0469, -0.0293, -0.0406, -0.1349,\n",
              "                       0.0369, -0.0840, -0.1369, -0.1651,  0.1544,  0.1826, -0.0818, -0.0877,\n",
              "                      -0.0865,  0.0392,  0.0888,  0.0472,  0.0816,  0.2103,  0.1581, -0.1774,\n",
              "                      -0.0480,  0.0443,  0.0648,  0.0193,  0.1277, -0.1794, -0.1159, -0.0569,\n",
              "                      -0.0196, -0.0894,  0.1500,  0.0517])),\n",
              "             ('rnn.bias_hh_l1_reverse',\n",
              "              tensor([ 0.1292,  0.2929,  0.2059, -0.0204,  0.1587,  0.2894,  0.3435,  0.0076,\n",
              "                       0.1436, -0.0510, -0.1108,  0.0504,  0.2009,  0.0806, -0.0314,  0.3339,\n",
              "                       0.0566,  0.3034, -0.0202,  0.2009,  0.0688,  0.0166,  0.1160,  0.0175,\n",
              "                      -0.0077,  0.2003,  0.2332, -0.1812,  0.2757,  0.2112, -0.0539, -0.1352,\n",
              "                       0.2310, -0.1714,  0.1642,  0.0708,  0.2187, -0.0708,  0.1988, -0.2097,\n",
              "                      -0.0299, -0.0111, -0.2165, -0.0009,  0.1762,  0.1796, -0.0095,  0.1177,\n",
              "                      -0.0731,  0.1313,  0.0040, -0.2867, -0.2412,  0.1319, -0.0650, -0.0393,\n",
              "                      -0.1042, -0.0968,  0.2094, -0.0503])),\n",
              "             ('fc.weight',\n",
              "              tensor([[ 0.0878,  0.0143, -0.1347, -0.0806,  0.0156, -0.0902, -0.1592,  0.0515,\n",
              "                       -0.0790,  0.0098,  0.0745,  0.1030,  0.0450, -0.0778,  0.0215,  0.1381,\n",
              "                       -0.0950,  0.1211,  0.1195, -0.1032,  0.0089, -0.1774,  0.0811, -0.0608,\n",
              "                        0.2680,  0.2720, -0.1641, -0.2228, -0.1952, -0.1164,  0.0235, -0.1195,\n",
              "                       -0.1257,  0.1822,  0.0898, -0.2299,  0.1641, -0.0931,  0.2542, -0.1892]])),\n",
              "             ('fc.bias', tensor([-0.0043]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cpC5aImi_ht"
      },
      "source": [
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/STATE DICT OF SCRAPPED RNN DATASET/deprnnscrapped_state_dic\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8he7kNk_js2y"
      },
      "source": [
        "sentence = 'I hate this world.'"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cFx8MSJkzkz"
      },
      "source": [
        "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVxGmw_Ak04H"
      },
      "source": [
        "indexed = [TEXT.vocab.stoi[t] for t in tokenized]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_6e2nM5k3Pj"
      },
      "source": [
        "tensor = torch.LongTensor(indexed)"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUtSDoKxk46O"
      },
      "source": [
        "tensor = tensor.unsqueeze(1)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkW-U4sOk6ST"
      },
      "source": [
        "prediction = torch.sigmoid(model(tensor))"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrQYRHNXk8Gg",
        "outputId": "6fb40810-dac3-40db-eaa3-854c66049e58"
      },
      "source": [
        "if prediction.item() > 0.5:\n",
        "  print(\"Depression Symtomps Detected\")\n",
        "else:\n",
        "  print(\"No Symtoms Detected\")  \n",
        "\n",
        "prediction.item()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Depression Symtomps Detected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7420164942741394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4pl989MlDTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}