{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Copy of bidirectional_Multilayer_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yash2998chhabria/Rnn-Nlp/blob/master/Copy_of_bidirectional_Multilayer_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoRp3XtIx4j8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7b9c7066-e141-4c81-c2d9-d366a817f9da"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVOA24701iTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a42c3da-1210-4b52-900b-95119e00afd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmoRf88Kx4kD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "5f3550bf-bd91-4915-e5db-af5116aa4732"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLetcTBIx4kH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2225c594-ee17-4e5c-cbde-85cef13c17ea"
      },
      "source": [
        "import spacy \n",
        "\n",
        "spacy.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dG38OUDx4kL"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TERxs7Hwx4kP"
      },
      "source": [
        "### Twitter Sentiment Analysis Dataset\n",
        "Source: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mzb7hXKx4kQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dac228f6-c8ba-48e5-a726-2ab20c3d6ed3"
      },
      "source": [
        "tweets = pd.read_csv('/content/drive/My Drive/depressionrnn/test1307.csv', error_bad_lines = False)\n",
        "\n",
        "tweets = tweets.head(50000)\n",
        "tweets.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scotland Yard took photographs of the man wear...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have learnt to mask the bad sides.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The children were asleep and all was well, exc...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>making a mockery of me\\r\\nive been humiliated,...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Like usual I don't know Icelandic so I'm tryi...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  condition\n",
              "0  Scotland Yard took photographs of the man wear...        0.0\n",
              "1               I have learnt to mask the bad sides.        1.0\n",
              "2  The children were asleep and all was well, exc...        0.0\n",
              "3  making a mockery of me\\r\\nive been humiliated,...        1.0\n",
              "4  (Like usual I don't know Icelandic so I'm tryi...        0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCb31pkd2ECf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "993106ef-677d-405b-e821-92bda2d98b69"
      },
      "source": [
        "tweets = tweets.rename(index = str, columns = {'sentence': 'SentimentText', 'condition': 'Sentiment'})\n",
        "\n",
        "tweets.head()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Scotland Yard took photographs of the man wear...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have learnt to mask the bad sides.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The children were asleep and all was well, exc...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>making a mockery of me\\r\\nive been humiliated,...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Like usual I don't know Icelandic so I'm tryi...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  Scotland Yard took photographs of the man wear...        0.0\n",
              "1               I have learnt to mask the bad sides.        1.0\n",
              "2  The children were asleep and all was well, exc...        0.0\n",
              "3  making a mockery of me\\r\\nive been humiliated,...        1.0\n",
              "4  (Like usual I don't know Icelandic so I'm tryi...        0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUwa5k7lhVCp"
      },
      "source": [
        "tweets=tweets.dropna()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5EOmg0Ox4kV"
      },
      "source": [
        "The dataframe consists of 4 columns and we want to use only ‘Sentiment’ and ‘SentimentText’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jFTyrxYx4ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a8d324-9e98-43b5-c747-8109a2694d7c"
      },
      "source": [
        "tweets.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1306, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJU5RB4-x4ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2349ba35-6731-4bdb-87ab-0f179e4bd95c"
      },
      "source": [
        "tweets['Sentiment'].unique()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJZwLFefx4ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efddef0c-fbcc-4250-b1a6-541a8dda49c2"
      },
      "source": [
        "tweets.Sentiment.value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    776\n",
              "1.0    530\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaPiBGVWx4kl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "4c0c6224-8d53-4ecc-f2d0-0db5a9274da4"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
        "\n",
        "ax.set(xlabel='Labels')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0.5, 0, 'Labels')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRv0G_Vcx4kp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq4YwB1gx4kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f36e15-1a5f-4089-d3c4-0b492f0084dc"
      },
      "source": [
        "train.reset_index(drop=True), test.reset_index(drop=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                          SentimentText  Sentiment\n",
              " 0     I felt something that wasnt completely terribl...        1.0\n",
              " 1                              Fine I'll buzz it short.        0.0\n",
              " 2     Was he running from a perceived threat to his ...        0.0\n",
              " 3     All fun times, but when it gets to night in re...        1.0\n",
              " 4     just dont give up\\r\\n\\r\\n3. consume small amou...        0.0\n",
              " ...                                                 ...        ...\n",
              " 1039                  And if so, why didn't I see them?        1.0\n",
              " 1040             No one for me to visit or to visit me.        1.0\n",
              " 1041  What happened to the years?I've been lost in d...        1.0\n",
              " 1042  I know this is gonna be harsh but I envy peopl...        1.0\n",
              " 1043                         Why would you argument so?        0.0\n",
              " \n",
              " [1044 rows x 2 columns],\n",
              "                                          SentimentText  Sentiment\n",
              " 0    I have never been that low before, during thos...        1.0\n",
              " 1    It's strange to me that I could have such a ne...        1.0\n",
              " 2                                     First post here.        0.0\n",
              " 3                                          Schoolwork?        0.0\n",
              " 4    \\------\\r\\n\\r\\n**1600 miles away, a body is fo...        0.0\n",
              " ..                                                 ...        ...\n",
              " 257  He had stopped to chat to a British holidaymak...        0.0\n",
              " 258  Ditter went to access the personal effects fou...        0.0\n",
              " 259  That was until I saw online all of my friends ...        1.0\n",
              " 260                Wishing I could start over in life.        1.0\n",
              " 261  He had been dressed in military-style clothing...        0.0\n",
              " \n",
              " [262 rows x 2 columns])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_sbdtgyx4kx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4cf02de3-1e6e-4629-c2bb-e32a7c0e225d"
      },
      "source": [
        "\n",
        "train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>I felt something that wasnt completely terribl...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Fine I'll buzz it short.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>Was he running from a perceived threat to his ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>All fun times, but when it gets to night in re...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>just dont give up\\r\\n\\r\\n3. consume small amou...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095</th>\n",
              "      <td>And if so, why didn't I see them?</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>No one for me to visit or to visit me.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1295</th>\n",
              "      <td>What happened to the years?I've been lost in d...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>I know this is gonna be harsh but I envy peopl...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>Why would you argument so?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1044 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          SentimentText  Sentiment\n",
              "989   I felt something that wasnt completely terribl...        1.0\n",
              "10                             Fine I'll buzz it short.        0.0\n",
              "147   Was he running from a perceived threat to his ...        0.0\n",
              "346   All fun times, but when it gets to night in re...        1.0\n",
              "411   just dont give up\\r\\n\\r\\n3. consume small amou...        0.0\n",
              "...                                                 ...        ...\n",
              "1095                  And if so, why didn't I see them?        1.0\n",
              "1131             No one for me to visit or to visit me.        1.0\n",
              "1295  What happened to the years?I've been lost in d...        1.0\n",
              "860   I know this is gonna be harsh but I envy peopl...        1.0\n",
              "1127                         Why would you argument so?        0.0\n",
              "\n",
              "[1044 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhw7JuROx4k0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca77393b-e160-46c4-8527-62f13031f5a3"
      },
      "source": [
        "train.shape, test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1044, 2), (262, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4HH5FC3x4k4"
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/depressionrnn/train_tweets.csv', index=False)\n",
        "test.to_csv('/content/drive/My Drive/depressionrnn/test_tweets.csv', index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wr075uWx4k-"
      },
      "source": [
        "#### defining a funtion to clean the tweets by removing non alphanumeric character and links "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be72LhxIx4k_"
      },
      "source": [
        "def tweet_clean(text):\n",
        "    \n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
        "    \n",
        "    return text.strip()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDFTBXkux4lF"
      },
      "source": [
        "####  The tweet column (‘SentimentText’) needs processing and tokenization, so that it can be converted into indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bc5C4Ux4lF"
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "def tokenizer(s): \n",
        "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zgZS_Hhx4lI"
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
        "\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypKAesKgx4lL"
      },
      "source": [
        "datafields = [('SentimentText', TEXT),('Sentiment', LABEL)]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7pIHUJx4lO"
      },
      "source": [
        "#### We create torchtext dataset,TabularDataset which is specially designed to read csv and tsv files and process them. It is a wrapper around pytorch Dataset with additional features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHLL6FZqx4lP"
      },
      "source": [
        "trn, tst = torchtext.data.TabularDataset.splits(path = '/content/drive/My Drive/depressionrnn', \n",
        "                                                train = 'train_tweets.csv',\n",
        "                                                test = 'test_tweets.csv',    \n",
        "                                                format = 'csv',\n",
        "                                                skip_header = True,\n",
        "                                                fields = datafields)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5JKc5Oex4lR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bfb320-6de4-4a66-d2dc-fe3cb2d35c07"
      },
      "source": [
        "print(f'Number of training examples: {len(trn)}')\n",
        "print(f'Number of testing examples: {len(tst)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 1044\n",
            "Number of testing examples: 262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6p-X9lox4lU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b1a72a-ee9f-40c5-f571-1ae8ef4a77e6"
      },
      "source": [
        "vars(trn.examples[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '1.0',\n",
              " 'SentimentText': ['i',\n",
              "  'felt',\n",
              "  'something',\n",
              "  'that',\n",
              "  'was',\n",
              "  'nt',\n",
              "  'completely',\n",
              "  'terrible',\n",
              "  'but',\n",
              "  'it',\n",
              "  'went',\n",
              "  'away',\n",
              "  'overnight']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeR0dYB6x4lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09fa7a89-f7d6-4d78-a0f3-bd13b25db918"
      },
      "source": [
        "vars(tst.examples[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': '1.0',\n",
              " 'SentimentText': ['i',\n",
              "  'have',\n",
              "  'never',\n",
              "  'been',\n",
              "  'that',\n",
              "  'low',\n",
              "  'before',\n",
              "  'during',\n",
              "  'those',\n",
              "  'months',\n",
              "  'i',\n",
              "  'had',\n",
              "  'been',\n",
              "  'hospitalized',\n",
              "  'attempted',\n",
              "  'suicide',\n",
              "  'on',\n",
              "  'multiple',\n",
              "  'occasions',\n",
              "  'and',\n",
              "  'felt',\n",
              "  'nothing',\n",
              "  'except',\n",
              "  'the',\n",
              "  'urge',\n",
              "  'to',\n",
              "  'be',\n",
              "  'gone']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mm1hzZnx4lb"
      },
      "source": [
        "#### Load pretrained word vectors and build vocabulary\n",
        "Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors. We get these vectors simply by specifying which vectors we want and passing it as an argument to build_vocab. TorchText handles downloading the vectors and associating them with the correct words in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIwIxsI-x4lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0e0063-3728-4c19-ba03-2d436c0735cd"
      },
      "source": [
        "TEXT.build_vocab(trn, max_size=25000,\n",
        "                 vectors=\"glove.6B.100d\",\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(trn)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                          \n",
            "100%|█████████▉| 398618/400000 [00:23<00:00, 16409.88it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Vh_L2Kx4le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e9fbc4-c4db-42b8-88fc-a5b0a9d7fff3"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(50))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('i', 933), ('the', 748), ('to', 563), ('and', 557), ('a', 403), ('of', 319), ('in', 294), ('my', 241), ('that', 238), ('was', 234), ('it', 220), ('t', 170), ('me', 168), ('for', 161), ('but', 159), ('m', 157), ('s', 154), ('this', 150), ('on', 144), ('he', 143), ('is', 134), ('have', 130), ('so', 118), ('with', 111), ('just', 110), ('had', 109), ('not', 104), ('at', 98), ('be', 97), ('like', 97), ('been', 87), ('you', 83), ('from', 81), ('his', 77), ('as', 75), ('she', 71), ('no', 70), ('up', 68), ('were', 66), ('about', 65), ('they', 65), ('out', 64), ('can', 64), ('get', 62), ('all', 61), ('or', 61), ('do', 59), ('david', 59), ('one', 59), ('her', 59)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auLV18Hbx4lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adb12e6-f007-47e3-95c2-36a680a0314f"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'i', 'the', 'to', 'and', 'a', 'of', 'in', 'my']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDW7iqfVx4lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46eb2642-55ed-4471-9a6c-02779594cf95"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7fce28312730>, {'0.0': 0, '1.0': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hSgCTttx4ln"
      },
      "source": [
        "#### Loading the data in batches\n",
        "For data with variable length sentences torchtext provides BucketIterator() dataloader which is wrapper around pytorch Dataloader. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3iQxT-lx4lo"
      },
      "source": [
        "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "                                (trn, tst),\n",
        "                                batch_size = 64,\n",
        "                                sort_key=lambda x: len(x.SentimentText),\n",
        "                                sort_within_batch=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dahjcq_gx4lq"
      },
      "source": [
        "#### We'll be using a different RNN architecture called a Long Short-Term Memory (LSTM).\n",
        "\n",
        "<b>torch.nn.embedding</b> -A simple lookup table that stores embeddings of a fixed dictionary and size.This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "\n",
        "\n",
        "<b>LSTM</b> - Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n",
        "\n",
        "<b>bidirectional</b> - an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the last to the first (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$.\n",
        "\n",
        "\n",
        "<b>Dropout</b> - it works by randomly dropping out (setting to 0) neurons in a layer during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter and each neuron with dropout applied is considered indepenently.This helps in regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwvBK0Z3x4lr"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "       \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVvtaE2hx4lu"
      },
      "source": [
        "To ensure the pre-trained vectors can be loaded into the model, the EMBEDDING_DIM must be equal to that of the pre-trained GloVe vectors loaded earlier.\n",
        "\n",
        "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's pad_token attribute, which is pad by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgN2x9qHx4lu"
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "\n",
        "dropout = 0.5"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a1W_19xx4ly"
      },
      "source": [
        "model = RNN(input_dim, \n",
        "            embedding_dim, \n",
        "            hidden_dim, \n",
        "            output_dim, \n",
        "            n_layers, \n",
        "            bidirectional, \n",
        "            dropout)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSM3SFzKx4l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5564e2a-8fcf-449e-d5ba-6207fff279a5"
      },
      "source": [
        "model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(3375, 100)\n",
              "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkVuF1-Ex4l4"
      },
      "source": [
        "We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdiJ1iWSx4l4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce25cae0-8c4f-4107-c66b-ea03fcb34f15"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3375, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0m6qBQ-x4l7"
      },
      "source": [
        "We then replace the initial weights of the embedding layer with the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9QJVwAOx4l8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5951b1fe-4b93-4109-a7db-b48d834a46cf"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3187,  0.1261,  0.6007,  ...,  0.4883, -0.0990, -1.7508],\n",
              "        [-1.0207, -1.4380, -1.6308,  ...,  0.5947,  1.9670,  0.8700],\n",
              "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
              "        ...,\n",
              "        [ 0.7893,  0.6285, -0.4647,  ...,  0.0823,  0.8793, -0.0854],\n",
              "        [-0.7497,  0.3603,  0.9107,  ...,  0.1232,  0.2503, -0.1138],\n",
              "        [ 0.1238,  0.0467,  0.1646,  ..., -0.1151,  0.2209, -0.4480]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDxVFgKgx4mA"
      },
      "source": [
        "As our < unk > and < pad > token aren't in the pre-trained vocabulary they have been initialized using unk_init (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLrRP64wx4mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553f56eb-31a6-4b5a-f973-580fc3a52afb"
      },
      "source": [
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
            "        ...,\n",
            "        [ 0.7893,  0.6285, -0.4647,  ...,  0.0823,  0.8793, -0.0854],\n",
            "        [-0.7497,  0.3603,  0.9107,  ...,  0.1232,  0.2503, -0.1138],\n",
            "        [ 0.1238,  0.0467,  0.1646,  ..., -0.1151,  0.2209, -0.4480]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-hoeQfCx4mD"
      },
      "source": [
        "#### Train the Model\n",
        "\n",
        "We use Adam optimizer and loss function is BCEWithLogitLoss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PkBLvDzx4mE"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9CWMN0x4mJ"
      },
      "source": [
        "#### We define a function for training our model\n",
        "as we are now using dropout, we must remember to use model.train() to ensure the dropout is \"turned on\" while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2SzsrOOx4mJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum() / len(correct)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVPNCIGUx4mN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a2acc0-8baa-4465-9a12-0ea527e1bdb3"
      },
      "source": [
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "     \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch: 01 | Train Loss: 0.680 | Train Acc: 57.46% |\n",
            "| Epoch: 02 | Train Loss: 0.655 | Train Acc: 60.51% |\n",
            "| Epoch: 03 | Train Loss: 0.629 | Train Acc: 65.70% |\n",
            "| Epoch: 04 | Train Loss: 0.585 | Train Acc: 70.15% |\n",
            "| Epoch: 05 | Train Loss: 0.524 | Train Acc: 74.17% |\n",
            "| Epoch: 06 | Train Loss: 0.478 | Train Acc: 76.10% |\n",
            "| Epoch: 07 | Train Loss: 0.444 | Train Acc: 79.65% |\n",
            "| Epoch: 08 | Train Loss: 0.433 | Train Acc: 79.93% |\n",
            "| Epoch: 09 | Train Loss: 0.430 | Train Acc: 79.80% |\n",
            "| Epoch: 10 | Train Loss: 0.396 | Train Acc: 82.21% |\n",
            "| Epoch: 11 | Train Loss: 0.372 | Train Acc: 84.06% |\n",
            "| Epoch: 12 | Train Loss: 0.353 | Train Acc: 85.57% |\n",
            "| Epoch: 13 | Train Loss: 0.364 | Train Acc: 84.30% |\n",
            "| Epoch: 14 | Train Loss: 0.350 | Train Acc: 84.82% |\n",
            "| Epoch: 15 | Train Loss: 0.336 | Train Acc: 86.12% |\n",
            "| Epoch: 16 | Train Loss: 0.298 | Train Acc: 88.51% |\n",
            "| Epoch: 17 | Train Loss: 0.303 | Train Acc: 88.84% |\n",
            "| Epoch: 18 | Train Loss: 0.274 | Train Acc: 89.94% |\n",
            "| Epoch: 19 | Train Loss: 0.292 | Train Acc: 88.29% |\n",
            "| Epoch: 20 | Train Loss: 0.255 | Train Acc: 90.07% |\n",
            "| Epoch: 21 | Train Loss: 0.264 | Train Acc: 90.02% |\n",
            "| Epoch: 22 | Train Loss: 0.243 | Train Acc: 90.22% |\n",
            "| Epoch: 23 | Train Loss: 0.222 | Train Acc: 91.43% |\n",
            "| Epoch: 24 | Train Loss: 0.207 | Train Acc: 92.90% |\n",
            "| Epoch: 25 | Train Loss: 0.183 | Train Acc: 93.13% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6MfHUjEx4mR"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfuzS1z6x4mS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb2dce7-ef41-47a9-a2fe-71d9d00b2111"
      },
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for batch in test_iterator:\n",
        "\n",
        "        predictions = model(batch.SentimentText).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.Sentiment)\n",
        "\n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct = (rounded_preds == batch.Sentiment).float() \n",
        "        \n",
        "        acc = correct.sum()/len(correct)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "\n",
        "test_loss = epoch_loss / len(test_iterator)\n",
        "test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.543 | Test Acc: 80.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RX_savqx4mV"
      },
      "source": [
        "#### User Input\n",
        "We can now use our model to predict the sentiment of any sentence we give it.As it has been trained on tweets, the sentences provided should in a positive or a negative context.\n",
        "\n",
        "We are expecting tweets with a negative sentiment to return a value close to 1 and positive tweets to return a value close to 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8juRfr9x4mV"
      },
      "source": [
        "sentence = 'ive started to isolate myself from other people'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jvsMXlgx4mY"
      },
      "source": [
        "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okpg-GWEx4mb"
      },
      "source": [
        "indexed = [TEXT.vocab.stoi[t] for t in tokenized]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y22hVPzdx4mf"
      },
      "source": [
        "tensor = torch.LongTensor(indexed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ijxjMkjx4mi"
      },
      "source": [
        "tensor = tensor.unsqueeze(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37TEQqXxx4ml"
      },
      "source": [
        "prediction = torch.sigmoid(model(tensor))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFHYlXEYx4mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a41822e9-634b-45d7-f65c-3c0f42960c09"
      },
      "source": [
        "prediction.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9590845704078674"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryZVfYkYx4ms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d4e64dbf-004a-4db6-e5ab-4d0a9ea9978c"
      },
      "source": [
        "torch.save(model, '/content/drive/My Drive/depressionrnn/eightytwo.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLgGIaxVx4mv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5c74056c-7f26-4272-e6cd-c55531e003d9"
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/depressionrnn/eightytwo.pt')\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(2732, 100)\n",
              "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJctbX0oNv2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a444b62a-6db6-46a9-8584-8c4f4cc65914"
      },
      "source": [
        "list(model.parameters())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "         [-0.0121,  0.0084,  0.0072,  ..., -0.0214,  0.0021,  0.0088],\n",
              "         [-0.0465,  0.6516,  0.5970,  ..., -0.3768,  0.0361,  0.8100],\n",
              "         ...,\n",
              "         [-0.9414, -0.3731,  0.0856,  ...,  0.5145,  0.6110,  0.2623],\n",
              "         [-0.0049,  0.2128,  0.2771,  ..., -0.7220, -0.2222, -0.7113],\n",
              "         [ 0.1604,  0.0406,  0.2770,  ..., -0.0563,  0.1611, -0.4086]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0260, -0.1328, -0.0900,  ...,  0.0154,  0.1880, -0.1659],\n",
              "         [ 0.2192, -0.2068,  0.1037,  ...,  0.2077, -0.0103,  0.1045],\n",
              "         [ 0.1496,  0.1885, -0.1441,  ...,  0.1054,  0.1378,  0.0093],\n",
              "         ...,\n",
              "         [ 0.0652,  0.0515,  0.0417,  ..., -0.2111, -0.1928, -0.1890],\n",
              "         [-0.2067, -0.0950,  0.1700,  ...,  0.0932, -0.0494, -0.1517],\n",
              "         [-0.1024,  0.1381, -0.1033,  ..., -0.0849, -0.0933, -0.1329]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.2141, -0.0182, -0.2257,  ..., -0.0712, -0.0662,  0.2487],\n",
              "         [ 0.1506, -0.0693,  0.0642,  ...,  0.1697,  0.1942, -0.0419],\n",
              "         [ 0.0106,  0.0574,  0.1200,  ...,  0.1529,  0.0776, -0.2040],\n",
              "         ...,\n",
              "         [ 0.1781, -0.1035,  0.2386,  ...,  0.0811,  0.0817, -0.2048],\n",
              "         [-0.1254,  0.2066,  0.0615,  ...,  0.1238, -0.0843, -0.1782],\n",
              "         [ 0.1146,  0.0977,  0.0992,  ..., -0.1350,  0.1205,  0.1536]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.1145, -0.2185,  0.0575, -0.0323, -0.0433,  0.1085,  0.1575,  0.2568,\n",
              "          0.1444,  0.0135, -0.1836,  0.0119, -0.0218, -0.0197,  0.1897,  0.0057,\n",
              "          0.0568, -0.0402,  0.0342,  0.1041,  0.0928,  0.2439,  0.1839, -0.1011,\n",
              "         -0.1889,  0.1904,  0.0788,  0.1922, -0.0837, -0.0252,  0.2280, -0.0529,\n",
              "          0.1061,  0.0974,  0.1914, -0.1910,  0.2052, -0.1498, -0.0305,  0.0715,\n",
              "         -0.0331,  0.1631,  0.0651, -0.0124,  0.2180,  0.1341, -0.0039,  0.1667,\n",
              "         -0.0438, -0.1924,  0.0373, -0.0962, -0.0608, -0.2036,  0.1473,  0.1560,\n",
              "          0.1819, -0.2149, -0.1092,  0.1180], requires_grad=True), Parameter containing:\n",
              " tensor([-0.1197, -0.0464, -0.1215,  0.1903, -0.1873, -0.1899, -0.1128,  0.0964,\n",
              "          0.2351,  0.0700, -0.1249,  0.0015,  0.0636,  0.0704,  0.1707,  0.0600,\n",
              "         -0.0416,  0.2201, -0.1952,  0.0834,  0.0186,  0.1131, -0.0124, -0.1883,\n",
              "          0.1409,  0.1100, -0.0586,  0.0372,  0.1394,  0.1306, -0.0612, -0.0863,\n",
              "         -0.0317, -0.1106,  0.0045, -0.1989, -0.1143, -0.1079, -0.2008,  0.2067,\n",
              "         -0.2199, -0.1815, -0.1193, -0.1024,  0.1909, -0.1482,  0.0473, -0.0477,\n",
              "         -0.0221,  0.1640,  0.1082,  0.0514, -0.0606, -0.1253,  0.0275, -0.0118,\n",
              "         -0.0932,  0.1066, -0.1881, -0.1898], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0072,  0.0947,  0.1533,  ...,  0.1308,  0.0050, -0.0414],\n",
              "         [ 0.2029, -0.0331,  0.0720,  ..., -0.1341,  0.1138, -0.2197],\n",
              "         [-0.0814,  0.0644, -0.1140,  ...,  0.1655,  0.0289, -0.0111],\n",
              "         ...,\n",
              "         [-0.1252,  0.0490,  0.0536,  ..., -0.1152, -0.0868, -0.0919],\n",
              "         [ 0.1719, -0.1837, -0.0984,  ..., -0.0295,  0.0201,  0.1540],\n",
              "         [-0.1719, -0.0701,  0.0277,  ..., -0.1331, -0.0072, -0.1239]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.1341,  0.0351, -0.0256,  ..., -0.0192, -0.2792, -0.2708],\n",
              "         [ 0.1716,  0.0705, -0.1257,  ..., -0.1871, -0.1336,  0.1720],\n",
              "         [ 0.2677,  0.1055,  0.1654,  ..., -0.0215,  0.1827,  0.1499],\n",
              "         ...,\n",
              "         [ 0.0011, -0.0322,  0.0379,  ...,  0.2067,  0.2558, -0.0249],\n",
              "         [-0.1509,  0.0989, -0.0942,  ..., -0.1466,  0.0562, -0.1093],\n",
              "         [-0.0155, -0.0183, -0.3410,  ..., -0.1816,  0.1738,  0.0723]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0868, -0.0831, -0.1498,  0.0033,  0.1128, -0.1411,  0.0978,  0.1485,\n",
              "         -0.0440,  0.2829,  0.0668, -0.2106,  0.2728,  0.0544,  0.0082, -0.1970,\n",
              "          0.0499,  0.1021, -0.0704,  0.1397, -0.1074,  0.1314,  0.2621, -0.1725,\n",
              "          0.1368,  0.0903,  0.1663, -0.0570, -0.0424, -0.0838, -0.0526, -0.0010,\n",
              "         -0.1354,  0.1687,  0.1337,  0.1729, -0.1661,  0.1827,  0.1542,  0.1988,\n",
              "         -0.0791, -0.1567,  0.0276, -0.0424,  0.1675, -0.0778,  0.0658, -0.0033,\n",
              "          0.0177, -0.1063,  0.1386,  0.1597,  0.1818, -0.0261,  0.0083,  0.0213,\n",
              "          0.0372,  0.1001,  0.1188, -0.1541], requires_grad=True), Parameter containing:\n",
              " tensor([ 0.2571, -0.1732,  0.1775,  0.0120,  0.2627, -0.1719,  0.2068, -0.1478,\n",
              "          0.2402,  0.0078,  0.1264, -0.1153, -0.1068, -0.0504,  0.1265,  0.1409,\n",
              "         -0.0136,  0.0463, -0.1262,  0.2160,  0.1257,  0.2406,  0.1847, -0.0788,\n",
              "          0.0686,  0.0891,  0.2008,  0.1247, -0.1192, -0.2169, -0.1923,  0.0109,\n",
              "          0.2087,  0.1929,  0.0567,  0.1438,  0.1208,  0.1840, -0.0920,  0.0630,\n",
              "         -0.2176,  0.1927, -0.1328, -0.1625, -0.1233,  0.0031, -0.0303,  0.0764,\n",
              "          0.1237,  0.2253,  0.0117,  0.0675, -0.0342, -0.0391,  0.0123, -0.0775,\n",
              "          0.0351,  0.0026,  0.0152,  0.1352], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.1688,  0.0017,  0.0556,  ...,  0.0370,  0.1250, -0.2099],\n",
              "         [ 0.0982, -0.1947, -0.0468,  ...,  0.1485, -0.0959, -0.0158],\n",
              "         [ 0.1836, -0.0640,  0.0781,  ..., -0.2132, -0.1591, -0.0579],\n",
              "         ...,\n",
              "         [ 0.1618, -0.1518,  0.0494,  ..., -0.0690,  0.1192,  0.1197],\n",
              "         [-0.0536,  0.1106, -0.0187,  ..., -0.0465, -0.1631, -0.0861],\n",
              "         [-0.0148,  0.1610, -0.0954,  ...,  0.1430, -0.1957, -0.0248]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0271,  0.0527,  0.1524,  ..., -0.0323,  0.0413, -0.0260],\n",
              "         [ 0.2255, -0.1717, -0.2075,  ..., -0.1353,  0.2344,  0.1759],\n",
              "         [ 0.0568,  0.0888, -0.2126,  ...,  0.1847,  0.1227,  0.1381],\n",
              "         ...,\n",
              "         [-0.2051, -0.1841, -0.1897,  ..., -0.0968, -0.0230,  0.1157],\n",
              "         [ 0.0129, -0.0053, -0.1346,  ..., -0.1067, -0.1372, -0.0760],\n",
              "         [-0.0100,  0.0928,  0.0019,  ...,  0.0520,  0.0467, -0.1114]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0008,  0.1610, -0.0502, -0.0102, -0.1082,  0.0156, -0.0668,  0.1375,\n",
              "          0.1248, -0.1075, -0.0079, -0.0629,  0.1072, -0.2321, -0.1804, -0.0342,\n",
              "         -0.1987, -0.1258,  0.0143,  0.2344, -0.0208, -0.1711,  0.0298, -0.1338,\n",
              "         -0.1899,  0.0726,  0.1953,  0.1867,  0.0049,  0.2026, -0.0867, -0.1500,\n",
              "          0.1720,  0.0531, -0.1970,  0.1497,  0.0642, -0.1791,  0.1910, -0.0690,\n",
              "         -0.1832, -0.0727, -0.0123,  0.0232, -0.0768,  0.0398,  0.1400, -0.1700,\n",
              "         -0.1300, -0.0950, -0.0554,  0.0160, -0.1243,  0.0901, -0.0631, -0.2333,\n",
              "         -0.2080, -0.0572,  0.0490,  0.1476], requires_grad=True), Parameter containing:\n",
              " tensor([ 0.2228, -0.1904,  0.0445, -0.0161, -0.0865, -0.0438,  0.1235, -0.1032,\n",
              "         -0.0144,  0.0402, -0.0129, -0.2061,  0.1928, -0.2064, -0.1899, -0.1365,\n",
              "          0.1190,  0.1063, -0.0015, -0.1297,  0.0456,  0.0737,  0.0839,  0.1786,\n",
              "          0.1769,  0.1704,  0.1333, -0.1826,  0.1783,  0.1368,  0.0035, -0.0659,\n",
              "          0.2168, -0.1063,  0.1762, -0.1243, -0.0544, -0.0556,  0.2063,  0.0329,\n",
              "          0.0034, -0.1575,  0.1958, -0.2370, -0.1027, -0.1685,  0.0012, -0.1386,\n",
              "          0.1970,  0.1408,  0.2182, -0.0245, -0.0515, -0.0985,  0.2000, -0.1661,\n",
              "          0.1417,  0.0111, -0.1437,  0.1692], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.2805, -0.1550,  0.1015,  ...,  0.1291, -0.2753, -0.1074],\n",
              "         [-0.1517,  0.1644, -0.1591,  ..., -0.0922, -0.2475, -0.0590],\n",
              "         [-0.0683, -0.2303, -0.2203,  ..., -0.0163,  0.0415,  0.2109],\n",
              "         ...,\n",
              "         [ 0.0699,  0.2018, -0.1753,  ...,  0.1863,  0.0535,  0.2098],\n",
              "         [-0.0916, -0.0299,  0.1371,  ..., -0.2120, -0.2028,  0.0668],\n",
              "         [-0.0519, -0.0540,  0.0049,  ..., -0.1031,  0.0206,  0.1861]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0482,  0.1036,  0.2160,  ...,  0.0024, -0.0160, -0.1589],\n",
              "         [ 0.1612, -0.1813,  0.1270,  ...,  0.0989, -0.0289,  0.0449],\n",
              "         [ 0.0657, -0.1910,  0.0107,  ...,  0.1428,  0.0982,  0.1859],\n",
              "         ...,\n",
              "         [-0.2153, -0.1502, -0.2179,  ..., -0.1271, -0.1536,  0.1680],\n",
              "         [ 0.1537,  0.2544, -0.0483,  ...,  0.0537,  0.1135,  0.0227],\n",
              "         [ 0.0364, -0.1827, -0.1480,  ...,  0.2208, -0.1917,  0.0502]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.1196,  0.1316, -0.2957, -0.0650, -0.0438,  0.1730,  0.1987,  0.1960,\n",
              "          0.2146,  0.2874,  0.0125, -0.0725,  0.0214, -0.2138,  0.2710, -0.0956,\n",
              "         -0.0207, -0.1266, -0.0315,  0.0284, -0.2164, -0.0082, -0.0841,  0.1999,\n",
              "         -0.1750,  0.0489,  0.0863, -0.2099, -0.0980,  0.1344,  0.1094, -0.0694,\n",
              "         -0.0461, -0.1690, -0.1864, -0.3133,  0.0612, -0.0519,  0.0865,  0.0992,\n",
              "         -0.0102, -0.0381, -0.1368, -0.0356,  0.1253, -0.1025, -0.0610, -0.2222,\n",
              "          0.1033,  0.0428, -0.2088, -0.1936, -0.1245, -0.0659, -0.1021,  0.0291,\n",
              "          0.1240,  0.1954, -0.0468,  0.0801], requires_grad=True), Parameter containing:\n",
              " tensor([ 3.9530e-02,  7.7597e-02, -1.2821e-01, -5.5247e-02,  5.3619e-02,\n",
              "          1.1194e-01,  1.2491e-02, -1.6214e-02,  4.3664e-02,  2.2807e-01,\n",
              "         -1.1219e-01,  1.2996e-02,  6.8676e-02, -8.8899e-02, -1.5998e-01,\n",
              "          1.2516e-01, -2.5476e-04,  2.7115e-01,  2.0409e-01, -8.4546e-02,\n",
              "          1.2795e-01, -5.6558e-02,  2.3043e-01, -1.2669e-01,  1.3007e-01,\n",
              "         -2.6536e-01, -2.5109e-01, -1.4045e-01, -2.1391e-01,  4.4432e-02,\n",
              "          1.2845e-01,  2.5949e-02, -1.3900e-01, -1.1620e-01, -1.0720e-01,\n",
              "         -2.1555e-01, -4.6319e-02, -1.3976e-02,  1.6656e-02,  6.2095e-02,\n",
              "          4.4627e-02,  1.1286e-01,  1.1426e-01, -4.9669e-02, -2.1137e-01,\n",
              "          1.4150e-01, -1.2376e-01,  3.1707e-02,  1.3152e-01, -1.0171e-01,\n",
              "          7.3961e-02, -1.2856e-01, -2.1297e-01,  4.6295e-02, -2.1689e-01,\n",
              "          8.7043e-02, -1.3678e-01, -1.2591e-01, -1.9431e-01, -2.3191e-03],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.1338, -0.1150,  0.0118,  0.1642,  0.0194,  0.0537,  0.0943,  0.0916,\n",
              "           0.1203,  0.1120, -0.1447, -0.0106, -0.1182, -0.0163, -0.0284,  0.0335,\n",
              "           0.0554,  0.1588, -0.0589, -0.1364, -0.2413, -0.1492, -0.2083, -0.3191,\n",
              "          -0.2694,  0.1771, -0.1282,  0.1993, -0.1500, -0.2238,  0.1652, -0.3053,\n",
              "           0.2626,  0.0482,  0.1799,  0.1324, -0.1364,  0.2449, -0.1900,  0.1878]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0011], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fOLNZ3O03iq"
      },
      "source": [
        "Now We gotta build the second RNN to understand the relations between sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of2NWhpGoUbF"
      },
      "source": [
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fovP7kTc54RZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "728b519f-f620-4896-f9a0-a7a0f3d90f35"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zovsnOFx7RWZ"
      },
      "source": [
        "para = pd.read_csv('/content/drive/My Drive/depressionrnn/paradataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlEC8MNx8eFU"
      },
      "source": [
        "lmao=para[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv9taXVl9fy2"
      },
      "source": [
        "input_data = {'sentence':[],'condition':[]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV9MrFNJ7hln"
      },
      "source": [
        "for i in range(len(para)):\n",
        "  sent_tokens = sent_tokenize(para.loc[i, \"submission\"])\n",
        "  input_line=[]\n",
        "  for sentence in sent_tokens:\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    input_line.append(prediction.item())\n",
        "  input_data['sentence'].append(input_line)\n",
        "  input_data['condition'].append(para.loc[i, \"condition\"])\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDjViLZ-vpeS"
      },
      "source": [
        "l=0\n",
        "m=0\n",
        "for i in range(len(para)):\n",
        "  sent_tokens = sent_tokenize(para.loc[i, \"submission\"])\n",
        "  l=l+len(sent_tokens)\n",
        "m=l/i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IApgYdr2v8RB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c87ac0e-402e-4dd8-ae1d-791a3229dbd1"
      },
      "source": [
        "m"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.506864988558355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cm_Mrryxhkn"
      },
      "source": [
        "vec_sentence = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NPtFU5Ax7JH"
      },
      "source": [
        "for i in range(len(para)):\n",
        "  sent_tokens = sent_tokenize(para.loc[i, \"submission\"])\n",
        "  for i in range(len(sent_tokens)):\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sent_tokens[i])]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    vec_sentence.setdefault(i,[]).append(prediction.item())\n",
        "  if i<580:\n",
        "    for i in range(i+1,580):\n",
        "      vec_sentence.setdefault(i,[]).append(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr_S5aPA2gLc"
      },
      "source": [
        "vec_sent = pd.DataFrame(vec_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSxRVg-d9saQ"
      },
      "source": [
        "average_vec_sent=vec_sent.iloc[:,0:36]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEiiye0vA67K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "3cba3757-dec3-417c-876f-4cf862baa8dc"
      },
      "source": [
        "average_vec_sent.sample(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>0.028025</td>\n",
              "      <td>0.081970</td>\n",
              "      <td>0.018994</td>\n",
              "      <td>0.012862</td>\n",
              "      <td>0.015752</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.017066</td>\n",
              "      <td>0.013045</td>\n",
              "      <td>0.013256</td>\n",
              "      <td>0.262223</td>\n",
              "      <td>0.015231</td>\n",
              "      <td>0.011691</td>\n",
              "      <td>0.043369</td>\n",
              "      <td>0.039004</td>\n",
              "      <td>0.05912</td>\n",
              "      <td>0.010286</td>\n",
              "      <td>0.013013</td>\n",
              "      <td>0.017844</td>\n",
              "      <td>0.010274</td>\n",
              "      <td>0.010663</td>\n",
              "      <td>0.01253</td>\n",
              "      <td>0.015656</td>\n",
              "      <td>0.011054</td>\n",
              "      <td>0.070824</td>\n",
              "      <td>0.015913</td>\n",
              "      <td>0.018429</td>\n",
              "      <td>0.01183</td>\n",
              "      <td>0.014299</td>\n",
              "      <td>0.01082</td>\n",
              "      <td>0.01339</td>\n",
              "      <td>0.010616</td>\n",
              "      <td>0.034037</td>\n",
              "      <td>0.018488</td>\n",
              "      <td>0.0133</td>\n",
              "      <td>0.010109</td>\n",
              "      <td>0.120025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1737</th>\n",
              "      <td>0.335660</td>\n",
              "      <td>0.906271</td>\n",
              "      <td>0.958230</td>\n",
              "      <td>0.976691</td>\n",
              "      <td>0.254853</td>\n",
              "      <td>0.960349</td>\n",
              "      <td>0.095552</td>\n",
              "      <td>0.879995</td>\n",
              "      <td>0.580362</td>\n",
              "      <td>0.933612</td>\n",
              "      <td>0.968061</td>\n",
              "      <td>0.974498</td>\n",
              "      <td>0.835673</td>\n",
              "      <td>0.940369</td>\n",
              "      <td>0.94068</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...      33        34        35\n",
              "937   0.028025  0.081970  0.018994  ...  0.0133  0.010109  0.120025\n",
              "1737  0.335660  0.906271  0.958230  ...  0.0000  0.000000  0.000000\n",
              "\n",
              "[2 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFzjZiIAxawS"
      },
      "source": [
        "mean value of number of sentences = 36"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sCA7beq_0Ye"
      },
      "source": [
        "data_para_vectors = pd.DataFrame.from_dict(input_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woa7LHd-1RHL"
      },
      "source": [
        "sent_tokens = sent_tokenize(\"\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otzfui2-1cWE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e25a1915-4695-4a02-e6ae-4427bc777eee"
      },
      "source": [
        "input_data = []\n",
        "sent_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am fed up with my life.',\n",
              " 'There is nothing much left.',\n",
              " 'I want to die.',\n",
              " 'i like taking  walks by the  beach']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 366
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG7UZmyF6ASw"
      },
      "source": [
        "for sentence in sent_tokens:\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "  indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "  tensor = torch.LongTensor(indexed)\n",
        "  tensor = tensor.unsqueeze(1)\n",
        "  prediction = torch.sigmoid(model(tensor))\n",
        "  input_data.append(prediction.item())\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4t0Lj7W6qr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "38a37919-2089-4871-d437-a64585fec87d"
      },
      "source": [
        "input_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9255887269973755,\n",
              " 0.8959446549415588,\n",
              " 0.9133660793304443,\n",
              " 0.11528420448303223]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 368
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0wDQQLZD_Cw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAxAyaUnBrox",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9009f268-d2e8-4a04-962f-d403e0b2afba"
      },
      "source": [
        "data_para_vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.023430433124303818, 0.10041609406471252, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.7287686467170715, 0.13594579696655273, 0.07...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.019319090992212296, 0.8115354180335999, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.9652117490768433, 0.9601704478263855, 0.974...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.01882638968527317, 0.017171267420053482, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>[0.02131967432796955, 0.021098053082823753, 0....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>[0.021219639107584953, 0.0192734282463789, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>[0.18058808147907257, 0.8963363170623779, 0.93...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>[0.2600974440574646, 0.018560275435447693, 0.0...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>[0.8456486463546753, 0.8619685173034668, 0.920...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  condition\n",
              "0     [0.023430433124303818, 0.10041609406471252, 0....          0\n",
              "1     [0.7287686467170715, 0.13594579696655273, 0.07...          1\n",
              "2     [0.019319090992212296, 0.8115354180335999, 0.0...          0\n",
              "3     [0.9652117490768433, 0.9601704478263855, 0.974...          1\n",
              "4     [0.01882638968527317, 0.017171267420053482, 0....          0\n",
              "...                                                 ...        ...\n",
              "1744  [0.02131967432796955, 0.021098053082823753, 0....          0\n",
              "1745  [0.021219639107584953, 0.0192734282463789, 0.0...          0\n",
              "1746  [0.18058808147907257, 0.8963363170623779, 0.93...          1\n",
              "1747  [0.2600974440574646, 0.018560275435447693, 0.0...          0\n",
              "1748  [0.8456486463546753, 0.8619685173034668, 0.920...          1\n",
              "\n",
              "[1749 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 405
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNhOVpVPAU_-"
      },
      "source": [
        "data_para_vectors.to_csv('/content/drive/My Drive/depressionrnn/para_vectors.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCQhqipPBkiT"
      },
      "source": [
        "data_para_vectors = pd.read_csv('/content/drive/My Drive/depressionrnn/para_vectors.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hsq7EK5KxGq"
      },
      "source": [
        "to_mer = data_para_vectors.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXAVapCXBgyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "95bad97a-cd7c-426a-b015-f23fda48f7e6"
      },
      "source": [
        "to_mer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1749 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      condition\n",
              "0             0\n",
              "1             1\n",
              "2             0\n",
              "3             1\n",
              "4             0\n",
              "...         ...\n",
              "1744          0\n",
              "1745          0\n",
              "1746          1\n",
              "1747          0\n",
              "1748          1\n",
              "\n",
              "[1749 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wd15lWiCZ5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "cd7b2764-3f31-4814-c27e-8f5ccfc13ff0"
      },
      "source": [
        "average_vec_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.021249</td>\n",
              "      <td>0.029183</td>\n",
              "      <td>0.009153</td>\n",
              "      <td>0.863360</td>\n",
              "      <td>0.437762</td>\n",
              "      <td>0.027893</td>\n",
              "      <td>0.034607</td>\n",
              "      <td>0.009567</td>\n",
              "      <td>0.539730</td>\n",
              "      <td>0.023512</td>\n",
              "      <td>0.010532</td>\n",
              "      <td>0.073112</td>\n",
              "      <td>0.032779</td>\n",
              "      <td>0.013092</td>\n",
              "      <td>0.015442</td>\n",
              "      <td>0.016392</td>\n",
              "      <td>0.055576</td>\n",
              "      <td>0.015903</td>\n",
              "      <td>0.011706</td>\n",
              "      <td>0.011917</td>\n",
              "      <td>0.014009</td>\n",
              "      <td>0.013470</td>\n",
              "      <td>0.077795</td>\n",
              "      <td>0.016448</td>\n",
              "      <td>0.687319</td>\n",
              "      <td>0.027469</td>\n",
              "      <td>0.062830</td>\n",
              "      <td>0.017083</td>\n",
              "      <td>0.335660</td>\n",
              "      <td>0.133792</td>\n",
              "      <td>0.470579</td>\n",
              "      <td>0.015406</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.034288</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.497672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.806606</td>\n",
              "      <td>0.128510</td>\n",
              "      <td>0.136395</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013921</td>\n",
              "      <td>0.856608</td>\n",
              "      <td>0.015094</td>\n",
              "      <td>0.032395</td>\n",
              "      <td>0.958026</td>\n",
              "      <td>0.903783</td>\n",
              "      <td>0.643045</td>\n",
              "      <td>0.772588</td>\n",
              "      <td>0.973150</td>\n",
              "      <td>0.067712</td>\n",
              "      <td>0.273345</td>\n",
              "      <td>0.818307</td>\n",
              "      <td>0.206419</td>\n",
              "      <td>0.153157</td>\n",
              "      <td>0.766966</td>\n",
              "      <td>0.018340</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>0.009675</td>\n",
              "      <td>0.019293</td>\n",
              "      <td>0.249056</td>\n",
              "      <td>0.024023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.942060</td>\n",
              "      <td>0.982114</td>\n",
              "      <td>0.983963</td>\n",
              "      <td>0.036931</td>\n",
              "      <td>0.643927</td>\n",
              "      <td>0.967281</td>\n",
              "      <td>0.946222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.013459</td>\n",
              "      <td>0.011142</td>\n",
              "      <td>0.014239</td>\n",
              "      <td>0.014808</td>\n",
              "      <td>0.021258</td>\n",
              "      <td>0.023457</td>\n",
              "      <td>0.014128</td>\n",
              "      <td>0.015280</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>0.010001</td>\n",
              "      <td>0.015189</td>\n",
              "      <td>0.011412</td>\n",
              "      <td>0.015431</td>\n",
              "      <td>0.017493</td>\n",
              "      <td>0.012073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1742</th>\n",
              "      <td>0.016779</td>\n",
              "      <td>0.016226</td>\n",
              "      <td>0.015499</td>\n",
              "      <td>0.013974</td>\n",
              "      <td>0.715402</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.011302</td>\n",
              "      <td>0.012160</td>\n",
              "      <td>0.965153</td>\n",
              "      <td>0.015847</td>\n",
              "      <td>0.861451</td>\n",
              "      <td>0.021216</td>\n",
              "      <td>0.095369</td>\n",
              "      <td>0.019180</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.038995</td>\n",
              "      <td>0.018170</td>\n",
              "      <td>0.019020</td>\n",
              "      <td>0.364901</td>\n",
              "      <td>0.010777</td>\n",
              "      <td>0.024120</td>\n",
              "      <td>0.018600</td>\n",
              "      <td>0.009852</td>\n",
              "      <td>0.026009</td>\n",
              "      <td>0.149471</td>\n",
              "      <td>0.045852</td>\n",
              "      <td>0.017224</td>\n",
              "      <td>0.012048</td>\n",
              "      <td>0.024373</td>\n",
              "      <td>0.017340</td>\n",
              "      <td>0.150561</td>\n",
              "      <td>0.013599</td>\n",
              "      <td>0.010900</td>\n",
              "      <td>0.013412</td>\n",
              "      <td>0.027110</td>\n",
              "      <td>0.011038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1743</th>\n",
              "      <td>0.014215</td>\n",
              "      <td>0.009749</td>\n",
              "      <td>0.015046</td>\n",
              "      <td>0.024823</td>\n",
              "      <td>0.017891</td>\n",
              "      <td>0.011946</td>\n",
              "      <td>0.011123</td>\n",
              "      <td>0.013225</td>\n",
              "      <td>0.010650</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.011576</td>\n",
              "      <td>0.012770</td>\n",
              "      <td>0.010118</td>\n",
              "      <td>0.013741</td>\n",
              "      <td>0.032768</td>\n",
              "      <td>0.251493</td>\n",
              "      <td>0.026829</td>\n",
              "      <td>0.011291</td>\n",
              "      <td>0.009629</td>\n",
              "      <td>0.013361</td>\n",
              "      <td>0.019585</td>\n",
              "      <td>0.109740</td>\n",
              "      <td>0.011663</td>\n",
              "      <td>0.026420</td>\n",
              "      <td>0.016733</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>0.015811</td>\n",
              "      <td>0.016686</td>\n",
              "      <td>0.012995</td>\n",
              "      <td>0.012610</td>\n",
              "      <td>0.019966</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.019783</td>\n",
              "      <td>0.013839</td>\n",
              "      <td>0.033524</td>\n",
              "      <td>0.024873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>0.253411</td>\n",
              "      <td>0.627633</td>\n",
              "      <td>0.968265</td>\n",
              "      <td>0.032800</td>\n",
              "      <td>0.768215</td>\n",
              "      <td>0.532675</td>\n",
              "      <td>0.935778</td>\n",
              "      <td>0.015314</td>\n",
              "      <td>0.154825</td>\n",
              "      <td>0.015683</td>\n",
              "      <td>0.014456</td>\n",
              "      <td>0.116663</td>\n",
              "      <td>0.119161</td>\n",
              "      <td>0.099314</td>\n",
              "      <td>0.021685</td>\n",
              "      <td>0.894721</td>\n",
              "      <td>0.019328</td>\n",
              "      <td>0.054429</td>\n",
              "      <td>0.774793</td>\n",
              "      <td>0.811992</td>\n",
              "      <td>0.046720</td>\n",
              "      <td>0.015591</td>\n",
              "      <td>0.013781</td>\n",
              "      <td>0.759258</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.152117</td>\n",
              "      <td>0.928234</td>\n",
              "      <td>0.013049</td>\n",
              "      <td>0.040625</td>\n",
              "      <td>0.031172</td>\n",
              "      <td>0.013982</td>\n",
              "      <td>0.107106</td>\n",
              "      <td>0.074915</td>\n",
              "      <td>0.969812</td>\n",
              "      <td>0.916486</td>\n",
              "      <td>0.016836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1745</th>\n",
              "      <td>0.343709</td>\n",
              "      <td>0.010765</td>\n",
              "      <td>0.044594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1746</th>\n",
              "      <td>0.953615</td>\n",
              "      <td>0.950967</td>\n",
              "      <td>0.966567</td>\n",
              "      <td>0.750556</td>\n",
              "      <td>0.900862</td>\n",
              "      <td>0.977851</td>\n",
              "      <td>0.591171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1747 rows × 36 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        33        34        35\n",
              "0     0.021249  0.029183  0.009153  ...  0.034288  0.015089  0.497672\n",
              "1     0.806606  0.128510  0.136395  ...  0.000000  0.000000  0.000000\n",
              "2     0.013921  0.856608  0.015094  ...  0.000000  0.000000  0.000000\n",
              "3     0.942060  0.982114  0.983963  ...  0.000000  0.000000  0.000000\n",
              "4     0.013459  0.011142  0.014239  ...  0.000000  0.000000  0.000000\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1742  0.016779  0.016226  0.015499  ...  0.013412  0.027110  0.011038\n",
              "1743  0.014215  0.009749  0.015046  ...  0.013839  0.033524  0.024873\n",
              "1744  0.253411  0.627633  0.968265  ...  0.969812  0.916486  0.016836\n",
              "1745  0.343709  0.010765  0.044594  ...  0.000000  0.000000  0.000000\n",
              "1746  0.953615  0.950967  0.966567  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[1747 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENojj7ozC2_X"
      },
      "source": [
        "sent_numeric = pd.merge(to_mer,average_vec_sent,left_index=True,right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogJXGaEsDagM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "5670f903-b553-4a40-c3f1-8eaee857a82c"
      },
      "source": [
        "sent_numeric.sample(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>condition</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1105</th>\n",
              "      <td>0</td>\n",
              "      <td>0.009367</td>\n",
              "      <td>0.012783</td>\n",
              "      <td>0.022719</td>\n",
              "      <td>0.011057</td>\n",
              "      <td>0.024844</td>\n",
              "      <td>0.059161</td>\n",
              "      <td>0.022686</td>\n",
              "      <td>0.020094</td>\n",
              "      <td>0.014623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1363</th>\n",
              "      <td>1</td>\n",
              "      <td>0.971507</td>\n",
              "      <td>0.034623</td>\n",
              "      <td>0.096926</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>1</td>\n",
              "      <td>0.019303</td>\n",
              "      <td>0.016513</td>\n",
              "      <td>0.835452</td>\n",
              "      <td>0.376689</td>\n",
              "      <td>0.909730</td>\n",
              "      <td>0.353970</td>\n",
              "      <td>0.534320</td>\n",
              "      <td>0.458479</td>\n",
              "      <td>0.058625</td>\n",
              "      <td>0.025330</td>\n",
              "      <td>0.354170</td>\n",
              "      <td>0.601579</td>\n",
              "      <td>0.915218</td>\n",
              "      <td>0.325215</td>\n",
              "      <td>0.017765</td>\n",
              "      <td>0.875673</td>\n",
              "      <td>0.850846</td>\n",
              "      <td>0.129917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667</th>\n",
              "      <td>1</td>\n",
              "      <td>0.335660</td>\n",
              "      <td>0.535987</td>\n",
              "      <td>0.961520</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>1</td>\n",
              "      <td>0.443714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>1</td>\n",
              "      <td>0.975443</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>1</td>\n",
              "      <td>0.943144</td>\n",
              "      <td>0.969009</td>\n",
              "      <td>0.978386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>1</td>\n",
              "      <td>0.094357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1614</th>\n",
              "      <td>1</td>\n",
              "      <td>0.035284</td>\n",
              "      <td>0.014343</td>\n",
              "      <td>0.431196</td>\n",
              "      <td>0.560822</td>\n",
              "      <td>0.280693</td>\n",
              "      <td>0.041694</td>\n",
              "      <td>0.186130</td>\n",
              "      <td>0.955176</td>\n",
              "      <td>0.078177</td>\n",
              "      <td>0.018452</td>\n",
              "      <td>0.020039</td>\n",
              "      <td>0.013777</td>\n",
              "      <td>0.016458</td>\n",
              "      <td>0.017221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>0</td>\n",
              "      <td>0.026609</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>0</td>\n",
              "      <td>0.015303</td>\n",
              "      <td>0.012060</td>\n",
              "      <td>0.013602</td>\n",
              "      <td>0.014766</td>\n",
              "      <td>0.012323</td>\n",
              "      <td>0.124717</td>\n",
              "      <td>0.050177</td>\n",
              "      <td>0.969976</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>0.016261</td>\n",
              "      <td>0.893887</td>\n",
              "      <td>0.357342</td>\n",
              "      <td>0.729089</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1347</th>\n",
              "      <td>1</td>\n",
              "      <td>0.560700</td>\n",
              "      <td>0.965114</td>\n",
              "      <td>0.027036</td>\n",
              "      <td>0.255786</td>\n",
              "      <td>0.052929</td>\n",
              "      <td>0.939803</td>\n",
              "      <td>0.950516</td>\n",
              "      <td>0.977032</td>\n",
              "      <td>0.220102</td>\n",
              "      <td>0.599637</td>\n",
              "      <td>0.969576</td>\n",
              "      <td>0.975319</td>\n",
              "      <td>0.964634</td>\n",
              "      <td>0.811967</td>\n",
              "      <td>0.117007</td>\n",
              "      <td>0.962166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>0</td>\n",
              "      <td>0.014034</td>\n",
              "      <td>0.026443</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>0.010147</td>\n",
              "      <td>0.012837</td>\n",
              "      <td>0.019385</td>\n",
              "      <td>0.015992</td>\n",
              "      <td>0.016830</td>\n",
              "      <td>0.016757</td>\n",
              "      <td>0.009315</td>\n",
              "      <td>0.013213</td>\n",
              "      <td>0.013329</td>\n",
              "      <td>0.013229</td>\n",
              "      <td>0.010758</td>\n",
              "      <td>0.016610</td>\n",
              "      <td>0.010566</td>\n",
              "      <td>0.021310</td>\n",
              "      <td>0.011840</td>\n",
              "      <td>0.022477</td>\n",
              "      <td>0.015404</td>\n",
              "      <td>0.012082</td>\n",
              "      <td>0.008836</td>\n",
              "      <td>0.020409</td>\n",
              "      <td>0.015088</td>\n",
              "      <td>0.018728</td>\n",
              "      <td>0.009456</td>\n",
              "      <td>0.019436</td>\n",
              "      <td>0.009538</td>\n",
              "      <td>0.01064</td>\n",
              "      <td>0.031242</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>0</td>\n",
              "      <td>0.335660</td>\n",
              "      <td>0.650198</td>\n",
              "      <td>0.921947</td>\n",
              "      <td>0.963718</td>\n",
              "      <td>0.016407</td>\n",
              "      <td>0.935717</td>\n",
              "      <td>0.272578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>0</td>\n",
              "      <td>0.011687</td>\n",
              "      <td>0.716563</td>\n",
              "      <td>0.569213</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>1</td>\n",
              "      <td>0.985380</td>\n",
              "      <td>0.719494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>1</td>\n",
              "      <td>0.973112</td>\n",
              "      <td>0.964538</td>\n",
              "      <td>0.889511</td>\n",
              "      <td>0.062160</td>\n",
              "      <td>0.103579</td>\n",
              "      <td>0.942509</td>\n",
              "      <td>0.962216</td>\n",
              "      <td>0.977242</td>\n",
              "      <td>0.014913</td>\n",
              "      <td>0.856949</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>1</td>\n",
              "      <td>0.966941</td>\n",
              "      <td>0.880553</td>\n",
              "      <td>0.047840</td>\n",
              "      <td>0.896044</td>\n",
              "      <td>0.914918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>1</td>\n",
              "      <td>0.197245</td>\n",
              "      <td>0.020039</td>\n",
              "      <td>0.016048</td>\n",
              "      <td>0.156676</td>\n",
              "      <td>0.013268</td>\n",
              "      <td>0.841308</td>\n",
              "      <td>0.024111</td>\n",
              "      <td>0.044568</td>\n",
              "      <td>0.019142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121</th>\n",
              "      <td>0</td>\n",
              "      <td>0.977579</td>\n",
              "      <td>0.953604</td>\n",
              "      <td>0.956756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      condition         0         1         2  ...   32   33   34   35\n",
              "1105          0  0.009367  0.012783  0.022719  ...  0.0  0.0  0.0  0.0\n",
              "1363          1  0.971507  0.034623  0.096926  ...  0.0  0.0  0.0  0.0\n",
              "72            1  0.019303  0.016513  0.835452  ...  0.0  0.0  0.0  0.0\n",
              "667           1  0.335660  0.535987  0.961520  ...  0.0  0.0  0.0  0.0\n",
              "154           1  0.443714  0.000000  0.000000  ...  0.0  0.0  0.0  0.0\n",
              "467           1  0.975443  0.000000  0.000000  ...  0.0  0.0  0.0  0.0\n",
              "59            1  0.943144  0.969009  0.978386  ...  0.0  0.0  0.0  0.0\n",
              "1456          1  0.094357  0.000000  0.000000  ...  0.0  0.0  0.0  0.0\n",
              "1614          1  0.035284  0.014343  0.431196  ...  0.0  0.0  0.0  0.0\n",
              "432           0  0.026609  0.000000  0.000000  ...  0.0  0.0  0.0  0.0\n",
              "605           0  0.015303  0.012060  0.013602  ...  0.0  0.0  0.0  0.0\n",
              "1347          1  0.560700  0.965114  0.027036  ...  0.0  0.0  0.0  0.0\n",
              "175           0  0.014034  0.026443  0.013519  ...  0.0  0.0  0.0  0.0\n",
              "1444          0  0.335660  0.650198  0.921947  ...  0.0  0.0  0.0  0.0\n",
              "951           0  0.011687  0.716563  0.569213  ...  0.0  0.0  0.0  0.0\n",
              "244           1  0.985380  0.719494  0.000000  ...  0.0  0.0  0.0  0.0\n",
              "906           1  0.973112  0.964538  0.889511  ...  0.0  0.0  0.0  0.0\n",
              "325           1  0.966941  0.880553  0.047840  ...  0.0  0.0  0.0  0.0\n",
              "913           1  0.197245  0.020039  0.016048  ...  0.0  0.0  0.0  0.0\n",
              "1121          0  0.977579  0.953604  0.956756  ...  0.0  0.0  0.0  0.0\n",
              "\n",
              "[20 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmAZ-dlDDpNZ"
      },
      "source": [
        "sent_numeric.to_csv('/content/drive/My Drive/depressionrnn/sent_numeric.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO3yiDlDFs9o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}